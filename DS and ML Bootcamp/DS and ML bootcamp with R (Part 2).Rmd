---
output:
  html_document: default
  word_document: default
---

# Section 19: Machine Learning
## Lecture 88:Introduction to Machine Learning with R 

ISLR is recommended.
http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Sixth%20Printing.pdf

Machine learning is a method of data analysis that automates analytical model building. Using algorithms that iteratively learn from data, machine learning allow computers to find hiddent insights without being explicitly programmed where to look. 

- fraud detection
- web search results
- real-time ads on web pages
- credit scoring and next-best offers
- prediction of equipment failures
- new pricing models
- network intrusion detection |
- recommendation engines
- customer segmentation
- text sentiment analysis
- predicting customer churn
- pattern and image recognition
- email spam filtering
- financial modeling 

General machine learning process is 
1. data acquision
2. data cleaning 
3. model training & building
4. model testing 
5. model deployment


### Supervised learning
Supervised learning are trained using labeled examples, such as input where the desired output is known.The learning lagorithm receives a set of inputs along with the corresponding correct outputs, and the algorith learns by comparing its actual output with correct outputs to find errors. It then modifies the model acordingly.
- classification
- regression
- prediction
- gradient boosting

Supervised learning is commonly used in application where historical data predicts likely future events. 

### Unsupervised learning
Unsupervised learning is used against data that has no historical labels. The system is not told the "right answer". The algorithm must figure out what is bening shown. The goal is to explore the data and find some structure within.

Unsupervised learning can find the main attributes that separate customer segments from each other. Popular techniques include self-organizing maps, nearest-neighbor mapping, k-means clustering and singular value decomposition.

### Reinforcement learning
Reinforcement learning is often used fro robotics, gaming and navigation. With reinforcement learning, te algorithm discovers through trial and error which actions yield the greatest rewards.

# Section 20: Linear Regression 
## Lecture 88: History, Background and overview 

The linear model formula takes the form (y~x). To add more predictor variables, just use + sign i.e., (y~x+y). 

Example 
- model structure
model <- lm (log(PINCP,base=10) ~ AGEP + SEX + COW + SCHL, data=dtrain)
- predict 
dtest$predLogPINCP <- predict(model, newdata=dtest)
dtrain$predLogPINCP <- predict(model, newdata=dtrain)

## Lecture 89: Linear regression with R(part1)

Get the data from: 
http://archive.ics.uci.edu/ml/datasets/Student+Performance

```{r csv data load}
# student math data
df <- read.csv("student-mat.csv",sep=";")
class(df)
names(df)
summary(df)

# no na data is stored in the data frame
any(is.na(df))
str(df)
```

We plot data using ggplot2
```{r data visualization}
library(ggplot2)
library(ggthemes)
library(dplyr)

# data correlation detection
# num only (numerical columns only)
num.cols <- sapply(df,is.numeric)
# filter 
cor.data <- cor(df[,num.cols]) %>% print()

# data visualize of correlation

# package: corrgram, 
library(corrgram)
library(corrplot)

# corrplot - quickly visualize correlation
corrplot(cor.data, method='color')

# corrgram - pass into the data frame directly
corrgram(df)
# help("corrgram")

# ggplot visualize
ggplot(df, aes(x=G3))+geom_histogram(bins = 20, alpha=0.5, fill='blue')
```

## Lecture 90: Linear regression with R(part2)
Split data into train and test set
- we use install.packages('caTools)
```{r}
# caTools for data split

Math <- read.csv("student-mat.csv",sep=";")
head(Math)

# install.packages('caTools')
library(caTools)

# set a seed
set.seed(101)
```
 
Approach (1): caTools package
```{r}
sample <- sample.split(Math$G3,SplitRatio=0.7)
train <- subset(df,sample==TRUE)
test <- subset(Math,sample=FALSE)
```

Approach (2): simple version using sample
```{r}
# alternative - split of training/test set based on ISLR
library(magrittr)
train_set <- sample(395,197)
train_set
print(head(Math,subset=train_set))
print(head(Math,subset=!train_set))

# linear model fit - all variables
lm.fit <- lm(G3~.,data=Math, subset=train_set)
lm.fit <- lm(G3~., data=train)

print(summary(lm.fit))

# coefficient plot
library(coefplot)
coefplot(lm.fit)

# residual value extract
res <- residuals(lm.fit)
class(res)
res <- as.data.frame(res)
head(res)

# plot residual
plot(lm.fit$residuals)
ggplot(res,aes(x=res))+geom_histogram(fill="light blue")

```

## Lecture 91: Linear regression with R (Part3)

Advanced visualization can be applied for model interprettion.
1. Residual vs Fitted
2. Normal Q-Q
3. Scale-location
4. Residual vs Leverage
```{r}
plot(lm.fit)
```

The model is used for prediction using the following code.
```{r prediction}
# Prediction
pred <- predict(lm.fit,test)

# column bind
results <- cbind(pred,test$G3)
results <- as.data.frame(results)
colnames(results) <- c('predicted','actual')
head(results)
plot(results$predicted,results$actual)


# take care of negative values
to_zero <- function(x){
  if (x<0){
    return(0)
  }else{
    return(x)
    }
  }
# apply zero function
results$pred <- sapply(results$pred,to_zero)

# mean squared error
MSE <- mean ((results$actual-results$predicted)^2) %>% print()
MSE
# RMSE
print(MSE^0.5)

SSE <- sum((results$predicted - results$actual)^2)
SST <- sum((mean(Math$G3 - results$actual)^2))
R2 <- 1 - SSE/SST
R2
```

## Lecture 92: Linear Regression Project

We first explore data to conduct a linear regression.

### Get the data
donwload the data or just use the supplied csv in te repository. The data has a following features:
- datetime: hourly date + timestamp 
- season: 1 = sprint, 2=summer,3=fall,4=winter
- holiday: whether the day 
- workingday - whether the day is neither a weekend nor holiday
- weather -
1: Clear, Few clouds, Partly cloudy, Partly cloudy
2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog
- temp - temperature in Celsius
- atemp - "feels like" temperature in Celsius
- humidity - relative humidity
- windspeed - wind speed
- casual - number of non-registered user rentals initiated
- registered - number of registered user rentals initiated
- count - number of total rentals

```{r bike csv file load}
bike <- read.csv('bikeshare.csv')
head(bike)
```

The purpose of our analysis is to predict count variable in te data set.

### Exploratory Data Analysis
First we create a scatter plot of count vs temp.
- count: number of total rentals
- temp: temperature in Celsius
```{r plot visualization}
# ggplot visualization
library(ggplot2)

ggplot(data=bike,aes(x=temp,y=count))+geom_point(aes(color=temp,alpha=0.1))+geom_smooth()+theme_bw()
```

Next, We plot count versus datetime as a scatterplot  with a color gradient based on temparature. You'll need to convert the datetime column into POSIXct before plotting.
```{r color gradual setting in ggplot}
library(ggplot2)
ggplot(bike,aes(x=datetime,y=count))+geom_point(aes(color=temp,alpha=0.2))+scale_color_continuous(low="blue",high="orange")+theme_bw()

# reference 
# ggplot(bike,aes(datetime,count)) + geom_point(aes(color=temp),alpha=0.5)  + scale_color_continuous(low='#55D8CE',high='#FF6E2E') +theme_bw()

```

Hopefully, we notice two things:
(1) A seasonality to the data for winter and summer. 
(2) Bike rental counts are increasing in general. 

The above might present a problem with using a linear regression model if the data is non-linear. Let's have a quick overview of pros/cons of linear regression.

Pros:
- Simple to explain
- Highly interpretable
- Model training and prediction are fast
- No tuning is required (excluding regularization)
- Features don't need scaling
- Can perform well with a small number of observations
- Well-understood

Cons:
- Assumes a linear relationship between the features and the response
- Performance is (generally) not competitive with the best supervised learning methods due to high bias
- Can't automatically learn feature interactions

We'll keep this in mind as we continue on. Maybe when we lear more algorithms, we can come back to this with some new tools, for now we'll stick to linear regression.

Next, we will calculate correlation between temp and count.
```{r correlation calculation}
cor.data <- cor(bike[,c("temp","count")])
cor.data

library(corrgram)
library(corrplot)

# corrplot - quickly visualize correlation
corrplot(cor.data, method='color')

# corrgram - pass into the data frame directly
corrgram(bike)
# help("corrgram")

```

Let's explore te season data. Create a boxplot, with the y axis indicating count and the x axis begin a box for each season.


```{r Boxplot visualization}
ggplot(bike,aes(x=factor(season),y=count))+geom_boxplot(aes(colour=factor(season)))
```

With this visualization, a line cannot capture a non-linear relatinship. Plus, there are more rentals in winter than in spring. We know of thse issues because of the growth of rental count, this isn't due to the actual season.

### Feature engineering
A lot of times you'll need to use domain knowledge and experience to engineer and create new features. Let's go ahead and engineer some new features from the datetime column.

First, we create an hour column that takes the hour from the datetime column. You'll probably need to apply some function to the entire datetime column and reassign it.

Hint: time.stamp = bike$datetime[4], format(time.stamp,"%H")
```{r Feature Engineering - as.POSIXct()}

library(magrittr)
time.stamp <- bike$datetime[4] %>% as.Date() %>% print()
format(time.stamp,"%H")

#as.POSIXct() function - CONVERT to POSIXct()
bike$datetime <- as.POSIXct(bike$datetime)

# sapply()
bike$hour <- sapply(bike$datetime,function(x){format(x,"%H")})
head(bike)
```

Now we crate a scatterplot count versus hour, with color scale based on temp. Only use bike data where workingday==1. 
Optional additions:
- use the additional layer: scale_color_gradient(colors=c('color1',color2))
- Use position=positio_jitter(w=1,w=0) inside of geom_point and check out what it does. 

```{r}
library(dplyr)
library(ggplot2)

pl <- ggplot(filter(bike,workingday==1),aes(hour,count)) 
pl <- pl + geom_point(position=position_jitter(w=1, h=0),aes(color=temp),alpha=0.5)
pl <- pl + scale_color_gradientn(colours = c('dark blue','blue','light blue','light green','yellow','orange','red'))
pl + theme_bw()
```

Now, we create the same plot for non working days:
```{r}
pl <- ggplot(filter(bike,workingday==0),aes(hour,count)) 
pl <- pl + geom_point(position=position_jitter(w=1, h=0),aes(color=temp),alpha=0.8)
pl <- pl + scale_color_gradientn(colours = c('dark blue','blue','light blue','light green','yellow','orange','red'))
pl + theme_bw()
```

Working days have peak activity during the morning (~8am) and right after work gets out (~5pm), with some lunchtime activity. While the non-work days have a steady rise and fall for the afternoon. Now let's continue by trying to build a model, we'll begin by just looking at a single feature. 

### Building a linear model 
We use lm() to build a model that predicts count based solely on the temp feature, name it temp.model.

```{r linear model setting}
temp.model <- lm(count~temp,data=bike)
summary(temp.model)
```

We got 6.0462 as the intercept and 9.17 as the temp coefficient.

#### Intercept
- it is the value of y when x=0
- Thus, it is the estimated number of rentals when the temperature is 0.
- Note: it does not always make sense to interpret the intercept

#### coefficient
- It is the change in y divided by change in x, or the "slope"
- Thus, a temparature increase of 1 degree is associated with a rental increase of 9.17 bikes. 
- This is not a statement of causation.
- $$\beta_1$$ would be negative if an increase in temperature was associated with a degrease in rentals.

### Predictiing model
Next question is how many bike rentals would we predict if temperature was 25 degrees Celsius? Calculate this two was:
(1) Use the values we just got above
(2) Use predict() function
```{r predict model}
6.0462 + 9.17*25
predict(temp.model,data.frame(temp=c(25)))
```

WE use sapply as.numeric to change the hour column to a column of numeric values. 
```{r sapply - column level change}
bike$hour <- sapply(bike$hour,as.numeric)
```

Finally, we build a model that attemps to predict count based off the following features. Figure out if there is a way not to pass/write all these variables into the lm() funcion.
```{r linear model with multiple variables}
lm.model2 <- lm(count~.-casual-registered-datetime-atemp,data=bike)
summary(lm.model2)

library(coefplot)
coefplot(lm.model2)
```

A linear model like this one which uses OLS won't be able to take into account seasonality of the data, and will get thrown in the dataset, accidentally attributing it towards the winter season, instead of realizing its just overall demand growing. Later on, we'll see if other models may be a better fit for this sort of data.

# Section 22: Logistic Regression
## Lecture 95: Intoroduction
Logistic regression is used as method for classification
examples
- spam versus ham emails
- loan default (yes,no)
- disease diagnosis

### Background
We have seen regression problesm where we try to predict continuous values. Although the name maybe confusion at first, logistic regression asslows us to solve classification problems, where we are trying to predict discrete categories. The convention for binary classification is to have two classes 0 and 1. 

The sigmoid (aka logistic) function takes in any value and outputs it to be between 0 and 1. The sigmoid function is defined as below.
$$
\phi(z) = \frac{1}{1+e^{-z}}
$$
This means we can take our linear regression solution and place it into the Sigmoid function. We can set a cutoff point at 0.5, anything below it results class 0, anything above is class 1. 

We use the logistic function to output a value ranging from 0 to 1. Based off of this probability we assign a class.

### Model evaluation
We can use a confusion matrix to evaluate our model. Testing for disease 
- Type I error: false positive
- Type II error: false negative

## Lecture 96: Logistic regression with R (Part1)
We use titanic data for applying logistic regression model.
- Passenger ID
- pclass: Passenger class
- SibSp: Number of siblings on board/spouses
- Parch: parents/children on board
- Fare: ticket fare
- Embarked: Queens Town

```{r titanic csv file load}
titanic.train <- read.csv('titanic_train.csv')
head(titanic.train)
str(titanic.train)
dim(titanic.train)
```
By using Amelia's missmap package, we can see that age variable has multiple missing values. 
```{r Amelia / missmap}
# install.packages('Amelia')
# library(Amelia)
# missmap(titanic.train,main='Missing Map',col=c('yellow','black'),legend = FALSE)
```

Simply by bar chart, we can see the data distribution. 
- third class > first class > second class
- most passengers are 20s and 30s (children on board)

```{r data distribution}
library(ggplot2)
# Pclass distribution
ggplot(titanic.train,aes(x=Pclass))+geom_bar(aes(fill=factor(Pclass)))
# Sex distribution
ggplot(titanic.train,aes(x=Sex))+geom_bar(aes(fill=factor(Sex)))
#  age distribution
ggplot(titanic.train,aes(x=Age))+geom_histogram(bins=20,alpha=0.3,fill='dark blue')
# SibSp - no Siblings/Spouses
ggplot(titanic.train,aes(SibSp))+geom_bar()

ggplot(titanic.train,aes(x=Fare))+geom_histogram(fill="light blue")
```

Options to deal with missing values for clean and tidy data are:
(1) exclude all rows with missing value 
* For this case, the 170 rows are too much for exclusion.
(2) Supplement missing values with some values 
e.g., fill in average age by passenger class

We will practice option 2.
```{r visualization of Pclass/Age distribution}
pl <- ggplot(titanic.train,aes(x=Pclass,y=Age))
pl <- pl + geom_boxplot(aes(group=Pclass,fill=factor(Pclass),alpha=0.4))
pl
pl + scale_y_continuous(breaks~seq(min(0),max(80),by=2))
```

```{r Inputation of age based on Pclass}
impute_age <- function(age,class){
    out <- age
    for (i in 1:length(age)){
        
        if (is.na(age[i])){

            if (class[i] == 1){
                out[i] <- 37

            }else if (class[i] == 2){
                out[i] <- 29

            }else{
                out[i] <- 24
            }
        }else{
            out[i]<-age[i]
        }
    }
    return(out)
}
```

## Lecture 97: Logistic Regression with R (Part 2)

As continued from the previous lecture, we supplement average age values in the N/A rows in the train set.By doing this, we don't have any row with missing values.

```{r incorporation of average age into N/A figures}

fixed.ages <- impute_age(titanic.train$Age,titanic.train$Pclass)
titanic.train$Age <- fixed.ages

# missmap(titanic.train,main="Imputation Check") #N/A values wre supplemented
```

As we do have several variables out of the analysis, we exclude those variables from the dataset.
```{r titanic data cleaning}
str(titanic.train)

library(dplyr)
titanic.train <- select(titanic.train,-PassengerId,-Name,-Ticket,-Cabin)
str(titanic.train)
dim(titanic.train)
```

```{r titanic data class conversion}
titanic.train$Survived <- as.factor(titanic.train$Survived)
titanic.train$Pclass <- as.factor(titanic.train$Pclass)
titanic.train$Parch <- as.factor(titanic.train$Parch)
titanic.train$SibSp <- as.factor(titanic.train$SibSp)

str(titanic.train)
```

```{r log.model application}
log.model <- glm(Survived~.,family=binomial(link='logit'),data=titanic.train)
summary(log.model)

library(coefplot)
coefplot(log.model)
```

Next, we predict test data by splitting train data.
```{r titanic - train/test split}

library(caTools)
set.seed(101)
split <- sample.split(titanic.train$Survived,SplitRatio = 0.7)
final.train <- subset(titanic.train,split==TRUE)
final.test <- subset(titanic.train,splot=TRUE)

final.log.model <- glm(Survived~.,family=binomial(link='logit'),data=final.train)
summary(final.log.model)
```

```{r model prediction}
fitted.probabilities <- predict(final.log.model,final.test,type='response')

fitted.results <- ifelse(fitted.probabilities>0.5,1,0)
misClassError <- mean(fitted.results!=final.test$Survived)
print(1-misClassError)

# confusion matrix
table(final.test$Survived,fitted.probabilities>0.5)
```

## Lecture 98: Logistic Regression Project
In this project, we will be owrking with the UCI adult dataset. We will be attempting to predict if people in the data set belong in a certain class by salary, either making <= 50k or >50k per year.

Typically, most of your time is spent cleaning data, not running the few lines of code that build your model, this proect will try to refrlect that by showing different issues that may arise when cleaning data. 

### Get the data
We first read in the adult.csv file and set it to a data frame called adult.
```{r adult csv load}
adult <- read.csv('adult_sal.csv')
head(adult)
colnames(adult)
```

The index has be repeated, thus we clean the data set by dropping this column, and check data overview.
```{r adult - culumn select}
library(dplyr)
adult <- select(adult,-X)

# data overview
head(adult)
str(adult)
summary(adult)
```

### Data Cleaning
Notice that we have a lot of columns that are categorical factors, however, a lot of these columns have too many factors than may be necessary. In this data cleaning section, we'll try to clean these columns up by reducing the number of factors. 

#### Type_employer column
First, we use table() to check the frequency of the type_employer column.
- The number of Null values in the type_employer: 1836
- Two smallest groups: Never-worked, Without-pay

```{r}
table(adult$type_employer)
```

We combine these two smallest groups into a single group called "Unemployed". There are lots of ways to do this, so feel free to get creative. 
Hint: it maybe helpful to convert these objects into character data types (as.character() and then use sapply with a custom function)

```{r unemp function}
unemp <- function(job){
  job <- as.character(job)
  if (job=='Never-worked' | job=='Without-pay'){
    return('Unemployed')
  } else{}
  return(job)
}
```

```{r unemployed definition}
adult$type_employer <- sapply(adult$type_employer,unemp)
table(adult$type_employer)
```

What other columns are suitable for combining? Combine state and local gov jobs into a category called SL-gov and combine self-employed jobs into a categorry called self-emp.

```{r}
group_emp <- function(job){
  if(job=='local-gov' |job=='State-gov'){
    return('SL-gov')
  } else if (job=='Self-emp-inc' | job=='Self-emp-not-inc'){
    return('self-emp')
  }else{
    return(job)
  }
}

adult$type_employer <- sapply(adult$type_employer,group_emp)
table(adult$type_employer)
```

### Marital Column
Next, we use table() look at the marital column.
```{r}
table(adult$marital)
```

let's reduce this to three groups:
```{r}
group_marital <- function(marital){
  marrital <- as.character(marital)
  # not-married
  if(marital=='Separated' | marital=='Divorced' | marital=='Widowed'){
    return('Non-Married')
  # never-married
  } else if (marital=='Never-married'){
   return('Never-Married')
    # married
  } else{
    return('Married')
  }
}

adult$marital <- sapply(adult$marital,group_marital)
table(adult$marital)
```

### Country Column
We check the country column using table()
```{r}
table(adult$country)
```

Group these contires together however we see fit. We have flexibilityhere because there is no right/wrong way to do this, possible group by continents. We should be able to reduce the number of groups here significantly though.
```{r}
levels(adult$country)
```

```{r}
Asia <- c('China','Hong','India','Iran','Cambodia','Japan', 'Laos' ,
          'Philippines' ,'Vietnam' ,'Taiwan', 'Thailand')

North.America <- c('Canada','United-States','Puerto-Rico' )

Europe <- c('England' ,'France', 'Germany' ,'Greece','Holand-Netherlands','Hungary',
            'Ireland','Italy','Poland','Portugal','Scotland','Yugoslavia')

Latin.and.South.America <- c('Columbia','Cuba','Dominican-Republic','Ecuador',
                             'El-Salvador','Guatemala','Haiti','Honduras',
                             'Mexico','Nicaragua','Outlying-US(Guam-USVI-etc)','Peru',
                            'Jamaica','Trinadad&Tobago')
Other <- c('South')
```

```{r group_country function}
library(magrittr)
group_country <- function(ctry){
    if (ctry %in% Asia){
        return('Asia')
    }else if (ctry %in% North.America){
        return('North.America')
    }else if (ctry %in% Europe){
        return('Europe')
    }else if (ctry %in% Latin.and.South.America){
        return('Latin.and.South.America')
    }else{
        return('Other')      
    }
}
```

```{r group_country application}
adult$country <- sapply(adult$country,group_country)
table(adult$country)
```

Once columns are cleared, we check the str() of adult again, and make sure any of the columns we changed have factor levels with factor().
```{r adult variable factor change}
adult$type_employer <- as.factor(adult$type_employer)
adult$country <- sapply(adult$country,factor)
adult$marital <- sapply(adult$marital,factor)

str(adult)
```

### Mssing data
Notice how we have data that is missing. 

We forst convert any celss with a '?' or a '?' value to a NA value. Hint: is.na maybe useful here or we can also use brackets with a conditional statement. 
```{r adult missing value conversion}
library(Amelia)
adult[adult=='?'] <- NA
```

Using table() on a column with NA values, we confirm those NA values are not shown, instead we will just see 0 for ?. 
```{r}
table(adult$type_emloyer)

adult$type_employer <- sapply(adult$type_employer,factor)
adult$country <- sapply(adult$country,factor)
adult$marital <- sapply(adult$marital,factor)
adult$occupation <- sapply(adult$occupation,factor)
```

We could have also done something like the below.
adult$type_employer = factor(adult$type_employer)

We check the distribution of missing values.USing missmap, we notice that heatmap pointing out missing values(NA). This gives us a quick glance at how much data is missing, in this case, not a whole lot (relatively speaking). We probably also notice that there is a bunch of y labels, get rid of them by running the command below. What is col=c('yellow','black') doing?

```{r}
library(Amelia)
missmap(adult)
missmap(adult,y.at=c(1),y.labels = c(''),col=c('yellow','black'))
```

Next, we use na.omit() to omit NA data from the adult data frame. Note, it really depends on the situation and the data to judge whether or not this is a good decision. We shouldn't always drop NA values.

```{r adult - na.omit NA}
adult <- na.omit(adult)
str(adult)
```

We again use missmap() to check that all the NA values were in fact dropped. 
```{r missmap after omit.na}
missmap(adult,y.at=c(1),y.labels=c(""),col=c("yellow","black"))
```

### EDA 
Although we've cleaned the data, we still have explored it using visualization. We use ggplot2 to create a histogram of ages, colored by income. 

```{r adult ggplot2}
str(adult)

library(ggplot2)
library(dplyr)

ggplot(adult,aes(x=age))+geom_histogram(aes(fill=income),color='black',binwidth=1)+theme_bw()
```

We plot a histgram of hours worked per week.
```{r adult histogram}
ggplot(adult,aes(x=hr_per_week))+geom_histogram(fill="light blue")+theme_bw()
```

We rename the country column to better reflect factor levels.
```{r region column definition}
# lots of ways to do this, could use dplyr as well

names(adult)[names(adult)=="country"] <- "region"
str(adult)
```

We create a barplot of region with the fill color defined by income class. Optional: Figure out how rotate the x axis text for readability.

```{r adult barbplot}
ggplot(adult,aes(x=region))+geom_bar(aes(fill=income),color='black')+theme_bw()+theme(axis.text.x=element_text(angle=90,hjust=1))
```

### Buliding a Model
Now it's time to build a model to classify people into two groups: above or below 50k in Salary.

### Logistic Regression
Details should be refered to the lecture of ISLR if we are fuzzy on any of this. 

Logistic regression is a type of classification model. In classification models, we attempt to predict the outcome of categorical depenedent variables, using one or more independent variables. The independent variables can be categorical or numerical. 

Logistic regression is based on the logistic function, which always takes values between 0 and 1. Replacing the dependent variable of the logistic function with a linear combination of dependent variables we intend to use for regression, we arrive at the formula for logstic  

We take a quick loo at the head() of adult to make sure we have a good overview before going into building the model.
```{r}
head(adult)
```

### Train test split
As a first step, we split the data into a train and test set using the caTools library as done in the previous lectures. Reference previous solution notebooks if we need a refresher.
```{r adult train/test split}
# caTools library
library(caTools)

# set a randome seed 
set.seed(101)

# spliit up the sample, basically randomlly assigns a booleans to a new column sample
sample <- sample.split(adult$income,SplitRatio = 0.7)

# Training data
train = subset(adult,sample==TRUE)
# Testing data
test=subset(adult,sample==FALSE)

head(train)
head(test)
```

### Fitting generalized linear models 
#### Description
glm is used to fit generalized linear models, specified by giving a symolic description of the linear predictor and a description of the error distribution.

#### Usage
glm(formula,family=gaussian,data,weights,subset...)

#### Details 
A typical predictor has the form response ~ terms where response is the (numeric) response vector and terms is a series of terms which specifies a linear predictor for response. For binomial and quasibinomial families the response can also be specified as a factor (when the first level denotes failure and all others success) or as a two-column matrix with the columns giving the numbers of successes and failures. A terms specification of the form first + second indicates all the terms in first together with all the terms in second with any duplicates removed.

A specification of the form first:second indicates the set of terms obtained by taking the interactions of all terms in first with all terms in second. The specification first*second indicates the cross of first and second. This is the same as first + second +
  first:second.

The terms in the formula will be re-ordered so that main effects come first, followed by the interactions, all second-order, all third-order and so on: to avoid this pass a terms object as the formula.

Non-NULL weights can be used to indicate that different observations have different dispersions (with the values in weights being inversely proportional to the dispersions); or equivalently, when the elements of weights are positive integers w_i, that each response y_i is the mean of w_i unit-weight observations. For a binomial GLM prior weights are used to give the number of trials when the response is the proportion of successes: they would rarely be used for a Poisson GLM.

glm.fit is the workhorse function: it is not normally called directly but can be more efficient where the response vector, design matrix and family have already been calculated.

If more than one of etastart, start and mustart is specified, the first in the list will be used. It is often advisable to supply starting values for a quasi family, and also for families with unusual links such as gaussian("log").

All of weights, subset, offset, etastart and mustart are evaluated in the same way as variables in formula, that is first in data and then in the environment of formula.

For the background to warning messages about ‘fitted probabilities numerically 0 or 1 occurred’ for binomial GLMs, see Venables & Ripley (2002, pp. 197–8).

#### Values
glm returns an object of class inheriting from "glm" which inherits from the class "lm". See later in this section. If a non-standard method is used, the object will also inherit from the class (if any) returned by that function.

The function summary (i.e., summary.glm) can be used to obtain or print a summary of the results and the function anova (i.e., anova.glm) to produce an analysis of variance table.

The generic accessor functions coefficients, effects, fitted.values and residuals can be used to extract various useful features of the value returned by glm.

weights extracts a vector of weights, one for each case in the fit (after subsetting and na.action).

An object of class "glm" is a list containing at least the following components:
- coefficient
- residual
- fitted.values
- rank
- family
- linear.predictors
- deviance
- aic
- null.deviance...

In addition, non-empty fits will have components qr, R and effects relating to the final weighted linear fit.

Objects of class "glm" are normally of class c("glm","lm"), that is inherit from class "lm", and well-designed methods for class "lm" will be applied to the weighted linear model at the final iteration of IWLS. However, care is needed, as extractor functions for class "glm" such as residuals and weights do not just pick out the component of the fit with the same name.

If a binomial glm model was specified by giving a two-column response, the weights returned by prior.weights are the total numbers of cases (factored by the supplied case weights) and the component y of the result is the proportion of successes.

#### Fitting functions
The argument method serves two purposes. One is to allow the model frame to be recreated with no fitting. The other is to allow the default fitting function glm.fit to be replaced by a function which takes the same arguments and uses a different fitting algorithm. If glm.fit is supplied as a character string it is used to search for a function of that name, starting in the stats namespace.

The class of the object return by the fitter (if any) will be prepended to the class returned by glm.

### Application of glm model 
We use all the features to train a glm() model on the training data set, pass the argument family=binomial(logit) into the glm function.
```{r adult model application}
adult.model = glm(income ~., family=binomial(logit),data=train)
summary(adult.model)
```

We  have still a lot of features. Some important, some not much. R comes with an awesome function called step(). The step() function iteratively tries to remove predictor variables from the model in an attempt to delete variables that do not significatnyl add to the fit. How does it do this? It uses AIC. 

### Choose a model by AIC in a stepwise algorithm
step() funciton seletcts a formula based model by AIC.

Usage: 
step(object, scope, scale = 0,
     direction = c("both", "backward", "forward"),
     trace = 1, keep = NULL, steps = 1000, k = 2, ...

#### Details
tep uses add1 and drop1 repeatedly; it will work for any method for which they work, and that is determined by having a valid method for extractAIC. When the additive constant can be chosen so that AIC is equal to Mallows' Cp, this is done and the tables are labelled appropriately.

The set of models searched is determined by the scope argument. The right-hand-side of its lower component is always included in the model, and right-hand-side of the model is included in the upper component. If scope is a single formula, it specifies the upper component, and the lower model is empty. If scope is missing, the initial model is used as the upper model.

Models specified by scope can be templates to update object as used by update.formula. So using . in a scope formula means ‘what is already there’, with .^2 indicating all interactions of existing terms.

There is a potential problem in using glm fits with a variable scale, as in that case the deviance is not simply related to the maximized log-likelihood. The "glm" method for function extractAIC makes the appropriate adjustment for a gaussian family, but may need to be amended for other cases. (The binomial and poisson families have fixed scale by default and do not correspond to a particular maximum-likelihood problem for variable scale.)

#### Values
the stepwise-selected model is returned, with up to two additional components. There is an "anova" component corresponding to the steps taken in the search, as well as a "keep" component if the keep= argument was supplied in the call. The "Resid. Dev" column of the analysis of deviance table refers to a constant minus twice the maximized log likelihood: it will be a deviance only in cases where a saturated model is well-defined (thus excluding lm, aov and survreg fits, for example).

#### Warning
The model fitting must apply the models to the same dataset. This may be a problem if there are missing values and R's default of na.action = na.omit is used. We suggest you remove the missing values first.

Calls to the function nobs are used to check that the number of observations involved in the fitting process remains unchanged.

#### Note
This function differs considerably from the function in S, which uses a number of approximations and does not in general compute the correct AIC.

This is a minimal implementation. Use stepAIC in package MASS for a wider range of object classes.

### Application of AIC to the model
We use new.model <- step(model.name) to use the step() function to create a new model.
```{r}
new.step.model <- step(adult.model)
summary(new.step.model)
```

We notice that the step() function kept all the features used previously.While we used the AIC criteria to compare models, there are other criteria we could have used. If you want you can try reading about the variable inflation factor (VIF) and vif() function to explore other options for comparison criteria. In the meantime, let's continue on and see how well our model performed against the test set.

We create a confusion matrix using the predict function with type="response" as an argument inside of that function.
```{r}
test$predicted.income=predict(adult.model,newdata=test,type='response')
table(test$income,test$predicted.income>0.5)
```

The accuracy of the model is 
```{r}
(6372+1422) / (6372+1423+548+872)
```

Other measures of performance, recall or precision are:
```{r}
# recall
6372/(6372+548)
# precision
6372 / (6372+872)
```

## LEcture 102: K Nearest Neighbors Introduction

Reference: ISLR Chapter 4

K Nearest Neighbors is a classification algorithm that operates on avery simple principle. It is best shown through example. Imagine we have some imaginary data on Dogs and Horses, with heights and weights. 


Training algorith:
1. Store all the data

Prediction algorithm:
1. Calculate the distance from x to all points in the data
2. Sort the points in the data by increasing distance from x
3. Predict the majority label of the "k" closest points

Choosing a K will affect what class a new point is assigned

Pros
- very simple
- training is trivial
- works with any number of lcasses
- Easy to add more data
- Few parameters: K and Distance metric

Cons
- high prediction costs
- not good with high dimensional data
- categorical feastures don't work well

## Lecture 103: K Nearest Neighbors (KNN) with R

### Get the data
Dataset: Caravan from ISLR package
```{r Caravan data load}
# install.packages("ISLR")
library(ISLR)

summary(Caravan)
any(is.na(Caravan))

library(Amelia)
missmap(Caravan)
head(Caravan)
```

Next, we look at each individual value.
```{r Caravan - test/train split}
var(Caravan[,1])
var(Caravan[,2])

purchase <- Caravan[,86]
standardized.Caravan <- scale(Caravan[,-86])
print(var(standardized.Caravan[,1]))
print(var(standardized.Caravan[,2]))

# Train test split
## Test set 
test.index <- 1:1000
test.data <- standardized.Caravan[test.index,]
test.purchase <- purchase[test.index]
# Train set
train.data <- standardized.Caravan[-test.index,]
train.purchase <- purchase[-test.index]
```

Based on the above training/test set, we fit a KNN model.
```{r Caravan - KNN model application}
library(class)

## KNN model
set.seed(101)
predicted.purchase <- knn(train.data,test.data,train.purchase,k=1)
head(predicted.purchase)

# misclass
mean(test.purchase != predicted.purchase)
misclass.error <- mean(test.purchase!=predicted.purchase)
print(misclass.error)


# k=3 case misclassification rate = 0.073
predicted.purchase <- knn(train.data,test.data,train.purchase,k=3)
mean(test.purchase != predicted.purchase)
# k=5 case
redicted.purchase <- knn(train.data,test.data,train.purchase,k=5)
mean(test.purchase != predicted.purchase)

#### Choosing a K value
predicted.purchase = NULL
error.rate = NULL

for(i in 1:20){
    set.seed(101)
    predicted.purchase = knn(train.data,test.data,train.purchase,k=i)
    error.rate[i] = mean(test.purchase != predicted.purchase)
}
print(error.rate)
```

```{r KNN visualization}
# visualize K elbow method
library(ggplot2)
k.values <- 1:20
error.df <- data.frame(error.rate,k.values)
ggplot(error.df,aes(k.values,error.rate))+geom_point()+geom_line(lty='dotted',color='red')
```

## Lecture 104: K Nearest Neighbors Project

since KNN is such a simple algorithm, we will use this project as a simple exercise to test the implemention of KNN. For this project, just follow along with the bolded instructions. It should be very simple, so at the end, we will have an additional optional bonus project.

### Get the data

We will use the famous iris data set for this project. It's a small data set with flower features that can be used to attempt to predict the species of an iris flower.

First, we use the ISLR library to get the iris data set, and check the heading of the data frame.
```{r iris data load}
library(ISLR)
head(iris)
str(iris)
```

### Standardize Data
In this case, th iris data set has all its features in the same order ofd magnitutde, but its good practice (especially with KNN) to standardize features in the data. Let's go ahead and do this even though its not necessary for this data. 

Use scale() to standaradize the feature columns of the iris data set. Set this standardized version of the data as a new variale.

```{r iris standardization}
var(iris[,1])
var(iris[,2])
var(iris[,3])
var(iris[,4])

species <- iris[,5]
standardized.iris <- scale(iris[,-5])

var(standardized.iris[,1])
var(standardized.iris[,2])
var(standardized.iris[,3])
var(standardized.iris[,4])

head(standardized.iris)
dim(standardized.iris)
```

### Train and Test splits 
Then, we use the caTools library to split the standardized data into train and test sets. Use 
```{r iris train/test split}

library(magrittr)
iris.train <-  sample(150,105)
iris.test <- (-iris.train)
iris.train.df <- standardized.iris[iris.train,] %>% as.data.frame()
iris.test.df <- standardized.iris[iris.test,] %>% as.data.frame()
head(iris.test.df)

library(caTools)
sample <- sample.split(iris$Species,SplitRatio=0.7)
train <- subset(iris,sample==TRUE)
test <- subset(iris,sample=FALSE)
```

### Build an KNN Model
We call the class library, and use the knn function to predict species of the test set, with k=1.
```{r iris KNN model}
# KNN: class package
library(class)

# knn model
set.seed(101)
predicted.species <- knn(train[1:4],test[1:4],train$Species,k=1)
predicted.species

# misclassification rate
mean(test$Species!=predicted.species)

```

### Choosing a K Value
Based on the above results, we do have quite small data, but considering the optimal k values. 
```{r iris k optimization}

predicted.species <- NULL
error.rate <- NULL

for(i in 1:10){
  set.seed(101)
  predicted.species = knn(train[,1:4],test[,1:4],train$Species,k=i)
  error.rate[i]=mean(test$Species!= predicted.species)
}
print(error.rate)
```

```{r iris KNN visualization}
# visualize K elbow method
library(ggplot2)
k.values <- 1:10
error.df <- data.frame(error.rate,k.values)
ggplot(error.df,aes(k.values,error.rate))+geom_point()+geom_line(lty='dotted',color='red')
```

We notice that the error drops to its lowest for k values between 2-6 (The above result is not consistent with the solution). Then it begins to jump backup again, this is due to how small the data set is. At k=10, we begine to approach seeking k=10% of the data, which is quite large.

http://archive.ics.uci.edu/ml/index.php

## Lecture 106: Tree Methods
Reference: ISLR Chapter 8

Decision tree
- nodes: split for the value of a certain attribute
- leaves: terminal nodes that predict the outcome
- root: the node that performs the first split
- leaves: terminal nodes that predict the outcome

Entropy and Information gain are the mathematical methods of choosing the best split. 
Entropy:
$$H(S) = - \Sigma_i p_i(S) log_2p_i(S)$$
Information Gain:
$$IG(S,A)0=H(s)-\Sigma \frac{|S_v|}{S}H(S_v)$$

To improve performance, we can use many trees with a randome sample of features chosen as the split.
- a new randome sample of features is chosen for every single tree at every single split.
- for classification, m is typically chosen to be the square root of p.

Suppose there is a one very strong feature in the data set. When using "bagged" trees, most of the trees will use that feature as the top split, resulting in an ensemble of similar trees that are highly correlated. 
- Averaging highly correlated quantities does not significantly reduce variance. 
- By randomly leaving out candidate features from each split, random forest decorrelates the trees, such that the averaging process can reduce the variance of the resulting model.

## Lecture 107: Decision Trees and Random Forests

Decision trees: rpart package
```{r rpart package}
library(rpart)
str(kyphosis)
head(kyphosis)
```

```{r rpart - model}
# model application
tree <- rpart(Kyphosis~.,data=kyphosis, ,method='class')
printcp(tree)

library(magrittr)
plot(tree,uniform=T, main='Kythosis Tree')
text(tree,use.n=T,all=T)
```

there are multiple functions available for decision tree model 
- printcp: display cp table
- plotcp: plot cross-validation results
- rsq.rpart: plot approximate R-squared and relative error for different splits, labels are only approproate for the anova method
- print:print results 
- summary: detailed results including surrogate splits
- plot: plot decision tree
- text:label the decision tree plot
- post: create postscript plot of decision tree

The above function can become better with package "rpart.plot"
```{r kythosis - prp}
library(rpart.plot)
prp(tree)
```

Randome free improves predictive accuracy by generating bootstrap trees  based on random samples of variables classifying a case using history in the new forest sighting a final predicted outcome by combining the results across all of the trees and in classification.

```{r random forest}
library(randomForest)

rf.model <- randomForest(Kyphosis~.,data=kyphosis)
print(rf.model)

rf.model$predicted
rf.model$ntree
rf.model$confusion
```

## Lecture 108: Deicision Trees and Randome Forests Project

For this project, we will be exploring the use of tree methods to classify schools as Private or Public based off their features. Let's start by getting the data which is included in the ISLR library, the College data frame.

A data frame with 777 observations on the following 18 variables.
-Private A factor with levels No and Yes indicating private or public university
- Apps Number of applications received
- Accept Number of applications accepted
- Enroll Number of new students enrolled
- Top10perc Pct. new students from top 10% of H.S. class
- Top25perc Pct. new students from top 25% of H.S. class
- F.Undergrad Number of fulltime undergraduates
- P.Undergrad Number of parttime undergraduates
- Outstate Out-of-state tuition
- Room.Board Room and board costs
- Books Estimated book costs
- Personal Estimated personal spending
- PhD Pct. of faculty with Ph.D.’s
- Terminal Pct. of faculty with terminal degree
- S.F.Ratio Student/faculty ratio
- perc.alumni Pct. alumni who donate
- Expend Instructional expenditure per student
- Grad.Rate Graduation rate

### Get the Data
We first call the ISLR library and check the head of College (a built-in data frame with ISLR, use data() to check this). Then reassign College to a dataframe called df.
```{r ISLR-college data load}
library(ISLR)
library(magrittr)
df <- College
head(df,10)
```

### Explotitation of Dataset (EDA)
We create a scatterplot of Grad.Rate versus Room.Board, colored by the Private column.
```{r College point_plot}
library(ggplot2)
ggplot(df,aes(x=Room.Board,y=Grad.Rate))+geom_point(aes(color=Private))
```

Then, we also create a histogram of full time undergrad students, color by Private.

```{r College histogram (F.undergad)}
ggplot(df,aes(x=F.Undergrad))+geom_histogram(aes(fill=Private),color='light grey')
```

Create a histogram of Grad.Rate colored by Private, 
```{r college histogra (Grad.rate)}
ggplot(df,aes(x=Grad.Rate))+geom_histogram(aes(fill=Private),color="light gray")
ggplot(df,aes(x=Grad.Rate))+geom_histogram(aes(fill=Private),color="light gray",bins=50)

```

What college had a Graduation rate of above 100%?
```{r}
library(magrittr)
head(df)

# subset() function
subset(df,Grad.Rate>100)
```

As we want to change the college's grad rate to 100%, we simply re-define the data frame.
```{r college df redifinition}
df['Cazenovia college','Grad.Rate'] <- 100
```


### Train Test Split
Next, we split the data into training and testing sets 70/30, by using caTools library for this.
```{r College train/test split}
library(caTools)
set.seed(101)

sample = sample.split(df$Private,SplitRatio=.70)
train=subset(df,sample==TRUE)
test=subset(df,sample==FALSE)
```

### Decision Tree
We use the rpart library to bild a decision tree to predict whether or not a school is Private. Remember to only build tree off the training data.
```{r College tree model application}
library(rpart)
tree <- rpart(Private~.,,method='class',data=train)
```

We use predict() to predict the Private label on the test data, and check the head of the predictied values. We could notice we do have two columns with the probabilities.  
```{r}
tree.preds <- predict(tree,test)
head(tree.preds)
```

We could turn these columns into one column to match the original Yes/No label for a private column.
```{r decision tree - predict conversion}
tree.preds <- as.data.frame(tree.preds)

joiner <- function(x){
  if (x>=0.5){
    return('Yes')
  }else {
    return('No')
  }
}

tree.preds$Private <- sapply(tree.preds$Yes,joiner)
head(tree.preds)
```

Based on the above, we use table() to create a confusion matrix of the tree model. We can plot such tree model by prp() function. 
```{r college confusion matrix/prp()}
# confusion matrix
table(tree.preds$Private,test$Private)

# plot by prp()
library(rpart.plot)
prp(tree)

```

### Random Forest
Let's build out a random forest model.
First, we call the randomForest package library, and use randomForest() to build out a model to predict Private class. Add importance=TRUE as a parameter in the model to find out what this does. 
```{r college rf model}
library(randomForest)
rf.model <- randomForest(Private~.,data=train,importance=T)
```

The model's confusion matrix on its own training set is,
```{r}
rf.model$confusion
```

To grab the feature importance with moel$importance, refer the reading for more info what Gini means.
```{r}
rf.model$importance
```

### Predictions
We use random forest model to predict the data set.
```{r College RF prediction}
p <- predict(rf.model,test)
table(p,test$Private)
```

It should have performed better than jut a single tree, how much better depends on whether you are emasuring recall, precision, or accuracy as the most important measure of the model.

# Section 28: Support Vector Machines
## Lecture 111: Introduction to Support Vector Machines
Reference: ISLR Chapter 11

Support vector machines (SVMs) are supervised learning models with associated learning algorithms that analyze data and recognizes patterns, used for classification and regression analysis.

Given a set of training examples, each marked for belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples into one category or the other, making it a non-probabilistic binary linear classifier. 

An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible.

New examples are then mapped into that same space and precited to belong to a category based on which side of the gap they fall on.

## Lecture 112: Support Vector Machine with R
### svm model application
```{r iris svm}
# iris dataset
library(ISLR)
print(head(iris))

library(e1071)
help('svm')

# svm model application
model <- svm(Species ~ .,data=iris)
summary(model)
```

### model prediction
```{r iris svm prediction}
pred.values <- predict(model,iris[1:4])
table(pred.values,iris[,5])
```

Parameters
- cost:basically allows the support vector machines to have what's known as a soft margin, parameter for the soft margin cost function which controls the influence of each individual support vector.
- gamma:free parameter of the Gaussian radial basis function (small gamma implies that the class of this support vector is going to have an influence on the side in the class of vector).

```{r svm tune}
# range:list of list and gamma
tune.results <- tune(svm,train.x=iris[1:4],train.y=iris[,5],kernel='radial',ranges=list(cost=c(0.1,1,10),gamma=c(0.5,1,2)))
summary(tune.results)

tune.results <- tune(svm,train.x=iris[1:4],train.y=iris[,5],kernel='radial',ranges=list(cost=c(0.5,1,1.5),gamma=c(0.1,0.5,0.7)))
summary(tune.results)
```

Based on the above results, cost=1.5,gamma=1 
```{r tuned.svm}
tuned.svm <- svm(Species~.,data=iris,kernel='radial',cost=1.5,gamma=0.1)
summary(tuned.svm)
```

## Lecture 114: Support Vector Machines Project
For this project, we will be exploring publicly available data from  LendingClub.com. Lending Club connects people who need money (borrowers) with people who have money (investors). Hopefully, as an investor you would want to invest in people who showed a profile of having a high probability of paying you back. We will try to create a model that will help predict this.

Lending club had a very interesting year in 2016, so let's check out some of their data and keep the context in mind. This data is from before they even went public.

We will use lending data from 2007-2010 and be trying to classify and predict whether or not the borrower paid back their loan in full. You can download the data from here or just use the csv already provided. It's recommended you use the csv provided as it has been cleaned of NA values.

Here are what the columns represent:
- credit.policy: 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise.
- purpose: The purpose of the loan (takes values "credit_card", "debt_consolidation", "educational", "major_purchase", "small_business", and "all_other").
- int.rate: The interest rate of the loan, as a proportion (a rate of 11% would be stored as 0.11). Borrowers judged by LendingClub.com to be more risky are assigned higher interest rates.
- installment: The monthly installments owed by the borrower if the loan is funded.
- log.annual.inc: The natural log of the self-reported annual income of the borrower.
- dti: The debt-to-income ratio of the borrower (amount of debt divided by annual income).
- fico: The FICO credit score of the borrower.
- days.with.cr.line: The number of days the borrower has had a credit line.
- revol.bal: The borrower's revolving balance (amount unpaid at the end of the credit card billing cycle).
- revol.util: The borrower's revolving line utilization rate (the amount of the credit line used relative to total credit available).
- inq.last.6mths: The borrower's number of inquiries by creditors in the last 6 months.
- delinq.2yrs: The number of times the borrower had been 30+ days past due on a payment in the past 2 years.
- pub.rec: The borrower's number of derogatory public records (bankruptcy filings, tax liens, or judgments).

### Data 
We open the loan_csv file and save it as a dataframe called loans.
```{r loan dataset csv}
loans <- read.csv('loan_data.csv')
```

We briefly check the aummary and structure of loans.
```{r summary of loan dataset}
str(loans)
summary(loans)
```

Next, we convert the following columns to categorical data using factor()
- inq.last.6mths 
- delinq.2yrs
- pub.rec
- not.fully.paid
- credit.policy

```{r catgorical data conversion}
loans$credit.policy <- factor(loans$credit.policy)
loans$inq.last.6mths <- factor(loans$inq.last.6mths)
loans$delinq.2yrs <- factor(loans$delinq.2yrs)
loans$pub.rec <- factor(loans$pub.rec)
loans$not.fully.paid <- factor(loans$not.fully.paid)

str(loans)
```

### EDA
Let's use ggplot2 to visualize the data. 
We first create a historam of fico scores colored by not.fully.paid

```{r loans - histogram}
library(ggplot2)

pl <- ggplot(loans,aes(x=fico))
pl <- pl + geom_histogram(aes(fill=not.fully.paid),color='light grey',bins=40,alpha=0.5)
pl + scale_fill_manual(values=c('green','red'))+theme_bw()
```

Then, we create a barplot of purpose counts, colored by not.fully.paid. Use position=dodge in the geom_bar argument.
```{r loans - barplot}
pl <- ggplot(loans,aes(x=factor(purpose)))
pl <- pl+geom_bar(aes(fill=not.fully.paid),position="dodge")
pl+theme_bw()+theme(axis.text.x=element_text(angle=90,hjust=1))
```

We also create a scatterplot of fico score versus int.rate. Does the trend make sense? Play around with the color scheme if you want.
```{r loans scatterplot}
ggplot(loans,aes(x=int.rate,y=fico))+geom_point()+theme_bw()

ggplot(loans,aes(x=int.rate,y=fico))+geom_point(aes(color=not.fully.paid),alpha=0.5)+theme_bw()
```

### Building the Model
Now its time to build a model.

#### Train and Test sets
We first split the data into training and test sets using the caTools library.
```{r loan test/train split}
library(caTools)
sample <- sample.split(loans$not.fully.paid,SplitRatio=0.8)
loans_train <- subset(loans,sample==T)
loans_test <- subset(loans,sample=F)
```

We call the e1071 library as shown in the lecture. We now use the svm() function to train a model on the training dataset.

```{r loans svm}
library(e1071)

# svm model application
loans_model <- svm(not.fully.paid~.,data=loans_train)
summary(loans_model)
```

We could use predict to predict new values from the test set suing the model. 
```{r loans svm prediction}
str(loans)

# svm model prediction
pred.values <- predict(loans_model,loans_test[1:13])
table(pred.values,loans_test[,14])
table(pred.values,loans_test$not.fully.paid)
```

### Tuning the model
We probably got some bad results. With the model classifying everything into one group. Lets tune the model to fix this issue.

We use the tune() function to test the different cost and gamma values. In the lecture, we showed how to do this by using train.x and train.y, but its usually simpler to just pass a formula. Try checking out help(tune) for mode details. This is the end of the project because tuning can take a long time (since its running a bunch of different models). 
```{r loans svm tuning}
tune.results <- tune(svm,train.x=not.fully.paid~.,data=loans_train,kernel='radial',ranges=list(cost=c(1,10),gamma=c(0.1,1)))
```

```{r tuned models}
model <- svm(not.fully.paid~., data=loans_train,cost=10,gamma=0.1)
predicted.values <- predict(model,loans_test[1:13])
table(predicted.values,loans_test$not.fully.paid)
```

# Section 30: K-Means Clustering
## Lecture 116: Introduction
Reference: ISLR Chapter 10

K-Means Clustering is an unsupervised learning algorithm that will attempt to group similar clusters together in the data.So what does a typical clustering problem liik like?
- cluster similar documents
- cluster customers based on features
- market segmentation
- identify similar physical groups

The K Means Algorithm
- choose a number of clusters "K"
- randomly assign each point to a cluster
- until clusters stop chaging, repeat the following:
(1) For each cluster, cpmpute the cluster centroid by taking the mean vector of points in the cluster
(2) Assing each data point to the cluster for which the centroid is the closest

There is no easy answer for choosing a best K value, but one way is the elbow method. 
- First of all, compute the sum of squared error (SSE) for some values of k.
- The SSE is defined as the sum of the squaired distance between each member of the cluster and its centroid.

If we plot k against the SSE, we will see that the error decreases as k gets larger; this is because when the number of clusters increases, they should be smaller, so distortion is also smaller.

The idea of the elbow method is to choose the k at which the SSe decreases abruptly. This produces an "elbow effect" in the graph, as we can see in the following picture:

## Lecture 117: K-Means Clustering with R

```{r iris data check}
library(ISLR)
head(iris)

library(ggplot2)
pl <- ggplot(iris,aes(Petal.Length,Petal.Width))+geom_point(aes(color=Species),size=4)
pl
```

We conduct K means clustering for iris dataset. 
```{r iris - K Means Clustering}
set.seed(101)

irisCluster <- kmeans(iris[,1:4],3,nstart=20)
irisCluster
```

When we evaluate K means clustering, we can simply use table() function. In addition, we can visualize such data, by cluster package.
```{r iris - KMC evaluation}
table(irisCluster$cluster,iris$Species)

# data visualization
library(cluster)
clusplot(iris,irisCluster$cluster,color=T,shade=T,labels=0,
         lines=0)
```

# Section 31: K Means Clustering Project
## Lecture 118: K Means Clustering Project

Usually when we dealing with an unsupervised learning problem, its difficult to get a good measure of how well the model performed. For this project, we will use data from the UCA archive based off of red and white wines. 

We will then add a label to the combined dataset, and will bring this label back later to see how well we can cluster the wine into groups.

### Get the Data

We first download two csv files from the UCL repository, then read.csv to open both datasets and set them as df1 and df2. Pay attention to what the separate(sep) is.

```{r wine csv data load}
df1 <- read.csv('winequality-red.csv',sep=";")
df2 <- read.csv('winequality-white.csv',sep=";")

str(df1)
str(df2)
```

Now, we add a label column to both df1 and df2 indicating a label "red" or "white".
```{r wine data label}
# rep() function
df1$label <- rep("red",1599)
df2$label <- rep("while",4898)

# using sapply with anon functions
df1$label <- sapply(df1$pH,function(x){'red'})
df2$label <- sapply(df2$pH,function(x){'white'})

head(df1)
head(df2)
```

We combine df1 and df2 into a single data frame called wine.
```{r wine dataset cobine}
names(df1)
colnames(df1)==colnames(df2)
wine <- rbind(df1,df2)
str(wine)
```

### EDA
Let's explore the data a bit and practice our ggplot2 skills.

#### histogram
We first create a histogram of residual sugar from the wine data. Color by red and white wines.
```{r wine resdual.sugar histogram}
library(ggplot2)
pl <- ggplot(wine,aes(x=residual.sugar))+geom_histogram(aes(fill=label),color='light grey',bins=50)
pl + scale_fill_manual(values=c('#ae4554','#faf7ea'))+theme_bw()
```

Next, we create a histogram of citric.acid from the wine data. Colored by red and white wines.

```{r wine citric.acid histogram}
pl <- ggplot(wine,aes(x=citric.acid))+geom_histogram(aes(fill=label),colo='light grey',bins=50)
# optional adding of fill colors
pl + scale_fill_manual(values=c('#ae4554','#faf7ea'))+theme_bw()
```

Lastly, we create a histogram of alchol from the wine data, colored by red and white wines.
```{r}
library(ggplot2)
pl <- ggplot(wine,aes(x=alcohol))+geom_histogram(aes(fill=label),color='light grey',bins=50)
# optional adding of fill color
pl + scale_fill_manual(values=c('#ae4554','#faf7ea'))+theme_bw()
```

#### scatterplot

Next, we create a scatterplot of residual.sugar versus citric.acid, color by red and white wine.
```{r wine scatterplot 1}
pl <- ggplot(wine,aes(x=citric.acid,y=residual.sugar))+geom_point(aes(color=label),alpha=0.1)
# optional adding of fill color
pl+scale_color_manual(values=c('#ae4554','#faf7ea'))+theme_dark()
```

We also create a scatterplot of volatile.acidity versus residual.sugar, color by rend and white wine.
```{r wine scatterplot 2}
pl <- ggplot(wine,aes(x=volatile.acidity,y=residual.sugar))+geom_point(aes(color=label),alpha=1)
# optional adding of fill colors
pl + scale_color_manual(values=c('#ae4554','#faf7ea'))+theme_dark()
```

To apply K means clustering, We grab the data without the label and call it clus.data.
```{r wine clus.data load}
library(magrittr)
str(wine)
clus.data <- wine[,1:12] %>% head()
head(clus.data)
```

### Building the clusters
We call the kmeans function on clus.data and assing the results to wine.cluster
```{r wine K-Means Clustering}
wine.cluster <- kmeans(wine[,1:12],2)
wine.cluster
print(wine.cluster$centers)
```

### Evaluating the Clusters
```{r}
table(wine.cluster$cluster,wine$label)

# data visualization
library(cluster)
clusplot(wine,wine.cluster$cluster,color=T,shade=T,labels=0,
         lines=0)

library(Amelia)
missmap(wine)
```

We can see that red is easier to cluster together, which makes sense given our previous visualizations. There seems to be a lot of noise with white wines, this could also be due to "Rose" wines being categorized as white wine, while still retaining the qualities of a red wine. Overall this makes sense since wine is essentially just fermented grape juice and the chemical measurements we were provided may not correlate well with whether or not the wine is red or white!

It's important to note here, that K-Means can only give you the clusters, it can't directly tell you what the labels should be, or even how many clusters you should have, we are just lucky to know we expected two types of wine. This is where domain knowledge really comes into play.

# Section 32: Natural Language Processing (NLP)

NLP is useful when
- we work for Google News and want to group news articles by topic
- we work for a legal firm and need to shift through thousands of pages of legal documents to find relevant one.  

NLP enables us to 
- compile documents
- get features from them
- compare those features 

<Simple example>
1. we have two documents, named "blue house", "red house"
2. Featurize based on word count
- "Blue House"->(red,blue,house) -> (0,1,1)
- "Red House" -> (red,blue,house) -> (1,0,1)

A document represented as a vector of word counts is called a "Bag of Words". We can use consine similarity on the vectors made to determine similarity:
$$
sim(A,B)=cos(\theta)=AB/||A||||B|| 
$$

Term frequency - importance of the term within that document
- TF(d,t)=Number of occurences of term t in document d

Inverse document frequency - importance of the term in the corpus
- IDF(t)=log(D/t), where 
* D=total number of documents, t=number of documents with the term

Methematically, TD-IDR is then expressed:

$$
W_{x,y}=tf_{x,y}*log(/frac{N}{df_x})
$$
TD-IDF: Term x within document y
- tf_x,y: frequency of x in y
- df_x: number of documents containing x
- N:total number of documents

## Lecture 121: Natural Language Processing with R(part1)
```{r twitter NLP package}
# package download
install.packages('tm')
install.packages('twitteR')
install.packages('wordcloud')
install.packages('RColorBrewer')
install.packages('e1017')
install.packages('class')
```

## Lecture 121: Natural Language Processing with R(part2)

We first use Twitter library using libraries.
{r}
library(twitteR)
library(tm)
library(wordcloud)
library(RColorBrewer)

- connect to twitter 
setup_twitter_oauth(ckey,skey,token,sectoken)

- search twitter
AI.tweets <- searchTwitter('AI',n=1000,lang='jp')
AI.text <- sapply(AI.tweets,function(x) x$getText())

- Clean text data
AI.text <- iconv(AI.text,'UTF-8','ASCII')
AI.corpus <- Corpus(VectorSource(AI.text))

- Document Term Matrix
term.doc.matrix <- TermDocumentMatrix(AI.corpus,control=list(removePunctuation=T, stopwords=c('AI',stopwords('english')),removeNumbers=T,tolower=T))

- Convert object into a matrix
ter.doc.matrix <- as.matrix(term.doc.matrix)

- Get word counts
word.freq <- sort(rowSums(term.doc.matrix),decreasing=T)
dm <- data.frame(word=names(word,freq),freq=word.freq)

- Create the wordcloud
wordcloud(dm$word,dm$freq,random.order=F,color=brewer.pal(8,'Dark2'))

# Section 33: Neural Nets
## Lecture 123: Introduction to Neural Nets
Neural networks are modeled after biological neural networks and attempt to allo computers to learn in a similar manner to humans - reinforcement learning.

- PAttern recognition
- Time series predictions
- Signal processing
- Anomaly detection
- Control

Neural Networks attempt to solve problems that would normally be easy for humans but hard for computers.

#### Perceptron
A perceptron consists of one or more imputs, a processor, and a singple output.A perceptron follows the "feed-foward" model, meaning inputs are sent into the neuron, are processed and result in an output. A perceptron process follows 4 main steps:
1. receive inputs
2. weight inputs
3. sum inputs
4. generate output

To actually train the perceptron, we use the following step:
1. Provide the perceptron with inputs for which there is known answer
2. Ask the perceptron to guess an answer
3. Compute the error (How far from the correct answer)
4. Adjust all the weights according to the error
5. Return to Step 1 and repeat

We repeat this until we reach an error we are satisfied.

## Lecture 125: Neural Networks with R

### Data set
We will use the popular Boston dataset from the MASS package, which describes some features for houses in Boston in 1978. 
- CRIM - per capita crime rate by town
- ZN - proportion of residential land zoned for lots over 25,000 sq.ft.
- INDUS - proportion of non-retail business acres per town.
- CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)
- NOX - nitric oxides concentration (parts per 10 million)
- RM - average number of rooms per dwelling
- AGE - proportion of owner-occupied units built prior to 1940
- DIS - weighted distances to five Boston employment centres
- RAD - index of accessibility to radial highways
- TAX - full-value property-tax rate per 10,000 dollars
- PTRATIO - pupil-teacher ratio by town
- B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
- LSTAT - % lower status of the population
- MEDV - Median value of owner-occupied homes in $1000's

```{r Boston NN}
# Neural Networks
library(MASS)
head(Boston)
```

```{r NN data overview}
# dataset overview
str(Boston)
# NA value
any(is.na(Boston))
```

By using neural net, We will be trying to predict the Median Value MEDV.

We must normalize the data set to apply neural networks model.
```{r NN data normalization}
# Maximum sets
maxs <- apply(Boston,2,max)
maxs

# Minimum sets
mins <- apply(Boston,2,min)
mins
```

### Dataset normalization
```{r NN scale normalization}
help("scale")
scaled.data <- scale(Boston,center=mins,scale=maxs-mins)
scaled <- as.data.frame(scaled.data)
head(scaled)
# first: center argument (column has corresponding values from center)
# scale: each value is divided by the scale
```

### Train, Test Split
```{r NN Boston test/split}
library(caTools)
split <- sample.split(scaled$medv,SplitRatio=0.7)
train <- subset(scaled,split==T)
test <- subset(scaled,split==F)
```

### Neural Net Model
We apply neural net to Boston dataset to estimate medv.

```{r Boston Neural Net application}

# install.packages("neuralnet")
library(neuralnet)

(n <- names(train))

f <- as.formula(paste("medv~",paste(n[!n %in% "medv"],collapse="+")))
f

nn <- neuralnet(f,data=train,hidden = c(5,3),linear.output=TRUE)
```

### Neural Net Visualization
We can plot out the model to see a very neat visualization with the weights on each connection.

The black lines show the connections between each layer and the weights on each connection while the blue lines show the bias term added in each step. The bias can be thought as the intercept of a linear model. The net is essentially a black box so we cannot say that much about the fitting, the weights and the model. Suffice to say that the training algorithm has converged and therefore the model is ready to be used.
```{r Boston - Neural Net Visualization}
plot(nn)
```

### Predictions using the Model
Now we can try to predict the values for the test set and calculate the MSE. Remember that the net will output a normalized prediction, so we need to scale it back in order to make a meaningful comparison (or just a simple prediction).
```{r Neural Net Prediction}
# compute predictions
predicted.nn.values <- compute(nn,test[1:13])
# list returned
str(predicted.nn.values)
```

```{r}
# Convert back to no-scaled predictions
true.predictions <- predicted.nn.values$net.result*(max(data$medv)-min(data$medv))+min(data$medv)
# Convert the test data
test.r <- (test$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
# Check the mean squaired error
MSE.nn <- sum((test.r - true.predictions)^2)/nrow(test)
MSE.nn
```

### Visualize Error
```{r}
error.df <- data.frame(test.r,true.predictions)
head(error.df)
```

```{r}
library(ggplot2)
ggplot(error.df,aes(x0test.r,y=true.predictions))+geom_point()+stat_smooth()
```

Looks like a few houses threw off our model, but overall its not looking too bad considering we're pretty much treating it like a total black box.

### Conclusion
Neural networks resemble black boxes a lot: explaining their outcome is much more difficult than explaining the outcome of simpler model such as a linear model. Therefore, depending on the kind of application you need, you might want to take into account this factor too. Furthermore, as you have seen above, extra care is needed to fit a neural network and small changes can lead to different results.

# Sectrion 34: Neural Nets Projects
## Lecture 125: Introduction

Let's wrap up this course by taking a a quick look at the effectiveness of Neural Nets!

We'll use the Bank Authentication Data Set from the UCI repository.

The data consists of 5 columns:
- variance of Wavelet Transformed image (continuous)
- skewness of Wavelet Transformed image (continuous)
- curtosis of Wavelet Transformed image (continuous)
- entropy of image (continuous)
- class (integer)

Where class indicates whether or not a Bank Note was authentic.

### Get the data
Use read.csv to read the bank_note_data.csv file.
```{r bank note - neural net}
df <- read.csv('bank_note_data.csv')
```

Check the head of the data frame and its structure.
```{r bank note - structure}
head(df)
str(df)
```

### EDA
Create whatever visualizations we are insterested in. We'll skip this step for the solutions notebook/video because the data isn't easily interpretable since its just statistical info on images.

### Train test split
Use the caTools library to split the data into training and test sets.
```{r bank note - test/train split}
library(caTools)
set.seed(101)
split=sample.split(df$Class,SplitRatio=.7)
train = subset(df,split==TRUE)
test=subset(df,split==FALSE)
```

Check the stucture of the train data and note class is still an int data type. We won't conver it to a factor for now, because neural net requires all numeric information.
```{r train data class}
str(train)
```

### Building the Neural Net
Call the neuralnet library to split the data into training and testing sets.
```{r bank note - neural net library}
library(neuralnet)
```

Use the neural net function to train a neural net, set linear.output = FALSE and choose 10 hidden neurons (hidden=10).
```{r bank note - neural net}
nn <- neuralnet(Class~Image.Var+Image.Skew+Image.Curt+Entropy,data=train,hidden=10,linear.output=FALSE)
```

### Predictions 
We use compute() to grab predictions using the nn model on the test set. 
```{r bank note neural net prediction}
predicted.nn.values <- compute(nn,test[,1:4])
head(predicted.nn.values$net.result)
```

Apply the round function to the predicted values so we see only 0s and 1s as your predicted classes.
```{r bank note prediction round}
predictions <- sapply(predicted.nn.values$net.result,round)
head(predictions)
```

Let's use table() to create a confusion matrix of the predictions versus the real data.
```{r bank note - comparison}
table(predictions,test$Class)
```

You should have noticed that you did very well! Almost suspiciously well! Let's check our results against a randomForest model!

### Comparing Models (Random Forest)
Run the code to set the class column of the data as a factor (randomForest needs it to be a factor, not an int like neural nets did. Then re-do the train/test split).
```{r bank note random forest}
df$Class <- as.factor(df$Class)
library(caTools)
set.seed(101)
split=sample.split(df$Class,SplitRatio=0.70)
train=subset(df,split==TRUE)
test=subset(df,split==FALSE)
str(train)
```

Then we create an randomForest model with the new adjusted training data, and use predict() to get the predicted values from the rd model.
```{r bank note- randomForest}
library(randomForest)
rf_model <- randomForest(Class~Image.Var+Image.Skew+Image.Curt+Entropy, data=train)
```

Use predict() to get the predicted values from the rf model.
```{r bank note - randomForest predict}
rf.pred <- predict(rf_model,test)
table(rf.pred,test$Class)
```

END




