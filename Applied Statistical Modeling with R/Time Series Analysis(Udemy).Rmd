---
title: "Time Series Analysis"
author: "Koji Mizumura"
date: "2018.07.25･"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

# Section 1
## LEcture 3: Main Functions

(1) Basic functions in R Base are:
- decompose()
- stl()
- arima()
- HolWinters()
- acf(), pacf()
- plot()
- ts()

(2) Add-on package - `forecast` package
```{r}
# package load
library(forecast)
library(ggplot2)
```

### Forecast structure 
Function `forecast()` + standard model = Forecasting

Standar model is (1) ARIMA, (2) Sesonal decomposition, (3) Exponential smoothing, and (4) simple models (e.g., naive mean).

Forecasting Results are always in the same structure regardless of the model(which are different from function `predict()`).

In the `Forecast` library, automatic parameter selection is available both for the ARIMA model and the exponential smoothing. 

### ARIMA models in Library `Forecast`
auto.arima(time_series) -> most suitable ARIMA model
`auto.arima` sets the complexity with the arguments `stepwise` and `approximation`. WE can get a list of possible models, as well as the information criteria.

Arima(time_series)
we could set the parameters manually (manual parameter selecion). In the manual selection, we look fro autocorrelation based on the `acf()` and `pacf()` plots. we adjust the model by `time_series` - `lags` until no more autocorrelation.

### Exponential smooothing in Library `Forecast`
Automatic function - `ets()`
manual functions: `ses()`, `hw()`, `holt()`

### Plotting with Library `Forrecast`

R Base Plots:
- `plot()`
- `monthplot()`
- `seasonplot()`

ggplot2 plots:
- `autoplot()`
- `ggmothplot()`
- `ggseasonplot()`

### Model comparison with Library `Forecast`
`accruacy()`
getting the model accuracy with a training and a test set.

`tsCV()`
time series cross validation for small datasets.

### Other packages 
we would use `getSymbols()`, `quantmod()` and `xts()` packages.
```{r}
# install.packages("quantmod")
# install.packages("xts")
```

## Lecture 4: Supporting Resources
R Time series Task View 
curated by Rob Hyndman (Blog on Otexts.org)
https://robjhyndman.com/

Vignetts 
some packages come with vignettes (description, PDF)

Free e-Books "Forecasting: Principles and Practice"
https://otexts.org/fpp2/index.html
https://robjhyndman.com/seminars/uwa2017/

R Community on Stackoverflow.com
https://stackoverflow.com/questions/tagged/r

## Lecture 5: Course Link List
Time Series Task View:
https://cran.r-project.org/web/views/TimeSeries.html

Blog, Ebook and Forecast Documentation by Rob Hyndman:
https://otexts.org/fpp2/intro.html

Stackoverflow Community:
https://stackoverflow.com/questions

Singapur Data of Project I:
https://docs.google.com/spreadsheets/d/1frieoKODnD9sX3VCZy5c3QAjBXMY-vN7k_I9gR-gcU8/pub
http://www.gapminder.org/data/

German Inflation Data of Project II:
https://www.statbureau.org/en/germany/inflation-tables

# Section 2: Project I Trending Data:Singapur Labor Force Participation Rate
## LEcture 6: Importing the data

Uneployment rate
- Used for propaganda purposes
- Easy to manipulate
- Who is unemployed?
- Who doesn't show up in the metric?

Labor Force Participatin Rate
- Harder to manipulate
- Ratio of people in the work force vs available people of a particular age range
- Factors for manipulation /bias
https://www.gapminder.org/data/

We need to compara things that are the same or similar level. Unbalanced comparisons require well thought out methodology to adjust for a mismatch. 
```{r}
# package load
library(forecast)
library(ggplot2)
library(quantmod)
library(xts)

# Import with scan
# "70.19999695	71.09999847	71.69999695	72.30000305	73.09999847	72.90000153	74.40000153	75.40000153	76	76.90000153	77.40000153	78.19999695	78.90000153	78.69999695	79	78	80	79.80000305	80.30000305	80.5	80.69999695	81.09999847	81.5	81.90000153	82.30000305	82.69999695	83.19999695	83.5"
# singapur=scan()
# singapur
singapur <-c(70.19999695,	71.09999847,	71.69999695,	72.30000305,	73.09999847,	72.90000153,	74.40000153,	75.40000153,	76,	76.90000153,	77.40000153,	78.19999695,	78.90000153,	78.69999695,	79,	78,	80,	79.80000305,	80.30000305,	80.5,	80.69999695,	81.09999847,	81.5,	81.90000153,	82.30000305,	82.69999695,	83.19999695,	83.5)
# item 28 are shown by this code 
singapur
singapur <- ts(singapur,start=1980)
singapur
plot(singapur,ylab="Labor Force Participation Rate")
```

From the above chart, the possible models are:
- ARIMA
- Holt linear trend method

However, one feature of this data is that the values cannnot exceed 100% in the model.There needs to be some press hold. Holt method has a nice damping parameter (holt()).

## Lecture 7: Mission Statement
From this lecture, we would focous on the theory and practical implementation through the four questions. 
- How to handle time series with trend?  
- Which methods are available?  
- What are the pitfalls?  
- How to visualize time series data?  

The process is 
1. Get the dataset  
2. Get the mission statement  
3. Work on the project  
4. Proceed with lectures and check the work  

The potential models are:  
- Linear trend model with `holt()`  
  - With damping paramter  
  - Without damping parameter  
- ARIMA model  

These models can be applied for forecasts, but with pitfalls, visualizations, alternatives. 

For applying the above models, we just need:   
(1) Libraries "forecast" and "ggplo2" + R base functions  
(2) Resources from the introductory section  

## Lecture 9: Exponential smoothing

Forecast package
- Simple exponential smooothing: `ses()`
- Holt's linear trend model]: `holt()` +damped
- Holt-Winters seasonal method: `hw()`
- Automated exponential smooothing: `ets()`

Holt linear trend model is:
$$
Y_{t+h|t} = l_t + hb_t  
$$  
-  $Y_{t+h|t}$: estimated forecast value at $t$ time point  
- $l$: level value at $t$ time point  
- $hb_t$: trend value at $t$ time point multiplied by $h$  

Should the model react only to recent data or should it consider older data as well?
-> Smoothing parameters

Smoothing parameters of a Holt Linear Trend Model are:  
1. $\alpha$:smoothing parameter for the level
2. $\beta$:smoothing parameter for the trend

The closer the smoothing parameter is to zero, the model becomes smoooth model (older date is consdered too), otherwise (close to 1) the model is the reactive model, heavily relying on recent data. 

## Lecture 10: The Holt Linear Trend Model 

In the Project I, we use `holt linear trend model`. 
In practice, we firstly activate the library `forecast`, and use the holt() function to create the model.   
- data=the time series to model  
- h=forecast length  
```{r 10.1}

# holt(data, h=x, damped=FALSE, level=c(80,95),fan=FALSE,initial=c("optimal","simple"),exponential=FALSE,alpha=NULL,beta=NULL,phi=NULL,lambda=NULL)

library(forecast)
holt_trend <- holt(singapur,h=5)
summary(holt_trend)
plot(holt_trend)
```

Most of the above arguments are the same for `ses()` and `hw()`.
By `summary()` function, we can obtain smoothing parameters, and initial state values. In the Singapur case, the participation cannot exceed

If we encounter simiular situation, exact parameters and thresholds. They know the literature, and experience, background information. 

For the above case, we use the damped argument. In the Holt linear trend model, the smoothing parameters are:   
- $\alpha$: smoothing parameter for the level  `alpha=`
- $\beta$: smoothing parameter for the trend  `beta=`
- $\phi$: damping parameter 0<$\phi$<1  `phi=`

We can easily adjust the model by the damping parameter.

If $\phi$ is one, its close to the original slope of the Holt trend model. If the $\phi$ is between 0.85 to 0.95, it is the generally recommended range of phi. If the $\phi$ is close to 0, its a flattened curve. 

**When the `damped` parameter is used, the slope of the trend cannnot be constant: It changes over time**

```{r 10.2}
#phi auto generated
plot(holt(singapur,h=15,damped=T))
# To see the generated value for phi: phi=0.96
summary(holt(singapur,h=15,damped=T))

# manual setting of phi
plot(holt(singapur,h=15,damped=T,phi=0.8))

# ARIMA auto generated
singapurarima=auto.arima(singapur)
summary(singapurarima)
```

## Lecture 11: The ARIMA model - Project I
The Box Jenkins models are standard modeling system for time series model. There are three parameters

- AR: Autoregressive such as seasonality, trend `P`
- |: Integreation - defferencing of the dataset `D`
- MA: Moving average - movement around a constant mean `Q`

The ARIMA model is very flexible for explaining:
- Random Walk
- Exponential Smooothing
- Autoregressive models such as AR(1), ARIMA(1,0,0)
- Moving average (MA(1), ARIMA(0,0,1))

The labour participation rate can be modeled by ARIMA. For that, we need to take into consideration, trending, autocorrelation (AR). Please note that, it seems there is no no moving average, seasonality, thus no differencing is needed.

With the `forecast` package, we can model the ARIMA model with manual parameter selection, and automatic parameter selection.
<`forecast` package>
- `Arima()`
- `auto.arima()`
```{r 11.1}
# ARIMA auto generated
singapurarima = auto.arima(singapur)
summary(singapurarima)

## ARIMA(1,1,0) with drift
## Summary output is same as the holt model
## The buttom shows that training set error measures

plot(forecast(singapurarima,h=5),ylab="Labor Participation Rate")

# The above setting might work for short term period
# Flatting curve needs to be designed

# Exact calculation of Arima parameters
auto.arima(singapur,stepwise = F,approximation = F)

```

## Lecture 12: Comparison between ggplot
The visualization should be simple. 

The plotting methods with `forecast` package are: 
- `plot()`:
quick and simple graphs with R base
- `autoplot()` and `autolayer()`:
Visually improved plots with libraries `ggplot2` and `forecast`.

The latest `forecast` package is in collaboration with the `ggplot` package. IN this lecture, we would visualize the Singapur data with the `ggplot` package.
```{r 12-1}
# model setting
# 1: Holttrend model
library(forecast)
holt_trend = holt(singapur,h=10)
holtdamped=holt(singapur,h=10,damped=T)
arimafore=forecast(auto.arima(singapur),h=10)
```

- Functions: `autoplot()` + `autolayer()`
- activate the libraries `ggplot` and `forecast`

<Procedure>
1. We first return a line graph(time series) by model. In the visualization, 
2. Models are added layer by later by using autolayer() from the `forecast` package.In addition, `$mean` extracts the forecast values from the model.
3. Axis labels are added with `+`.
4. The legend is added.

```{r 12-2}
library(ggplot2)
autoplot(singapur)+
  forecast::autolayer(holt_trend$mean,series="Holt Linear Trend")+
  forecast::autolayer(holtdamped$mean,series="Holt Damped Trend")+
  forecast::autolayer(arimafore$mean,series="ARIMA")+
  xlab("year")+ylab("Labour Force Participation Rage Age 25-54")+
  guides(colour=guide_legend(title="Forecast Method"))+
  theme(legend.position = c(0.8,0.2))+
  ggtitle("Singapur")+
  theme(plot.title = element_text(family="Times",hjust=0.5,color="blue",face="bold",size=15))
```

## Lecture 13:  In-Sample forecast vs actual data
The idea is to reconstruct the plot, and we compare the original data with in-sample values of the models.In comparision, we will color code the lines. Lastly, we adjust the tiles and labels.
```{r 13-1}
# package load
library(forecast)
library(ggplot2)

# models 
holttrend=holt(singapur,h=10)
holtdamped=holt(singapur,h=10,damped=T)
airmafore=forecast(auto.arima(singapur),h=10)

autoplot(singapur)+geom_line(size=2)+
  forecast::autolayer(holttrend$fitted,series="Holt Linear Trend",size=1.1)+
  forecast::autolayer(holtdamped$fitted,series="Holt Damped Trend",size=1.1)+
  forecast::autolayer(arimafore$fitted,series="ARIMA",size=1.1)+
  xlab("year")+ylab("Labour Force Participation Rate Age 25-54")+
  guides(colour=guide_legend(title="Forecast Method"))+
  theme(legend.position=c(0.8,0.2))+
  ggtitle("singapur")+theme(plot.title=element_text(family="Times",hjust=0.5,color="blue",face="bold",size=15))
```

# Section 3: Project II Seasonal data: monthly inflation rates of Germany
## Lecture 14: Getting familar with data 
Inflation rate is a measure of change in purchasing power per unit of money. It is also important to know how inflation moves along with other indicators.

We use a monthly inflation rate of Germany (Jany 2008 - Oct 2017) from ststbureau.org.
https://www.statbureau.org/
https://www.statbureau.org/en/germany/inflation-tables

The monthly rate should be more intuitive in the following comparison methods. 
- Oct 2017 vs Oct 2016:
Year on year rate (standard method)
- OCt 2017 vs Sept 2017:
Month on month rate (changes are easier to detect)

The inspecting the dataset comes from
1. No **trend** is present (constant mean)
2. **Seasonality** is present (seasonal decomposition, seasonal ARIMA, exponential smoothing)
3. Presence of **negative values** (multiplicative exponential smoothing models are excluded)
4. **Stable amplitude** (stable variance)

## Lecture 15: Importing the data
The general strucure of the dataset should be common. 

There are two things to keep in mind while importing 
- No headrs or row IDs 
- data needs to be arranged/sorted (preformating process)

Let's use a `scan()` function to import data.Only with this function, no timestamp is set yet. 
```{r 15-1}
# data load
# mydata = scan()

mydata <- c(-0.31,	0.41,	0.51,	-0.2,	0.61,	0.2,	0.61,	-0.3,	-0.1,	-0.2,	-0.51,	0.41, -0.51,	0.61,	-0.2,	0.1,	-0.1,	0.3,	0,	0.2,	-0.3,	0,	-0.1,	0.81, -0.6,	0.4,	0.5,	0.1,	-0.1,	0,	0.2,	0.1,	-0.1,	0.1,	0.1,	0.6, -0.2,	0.6,	0.59,	0,	0,	0.1,	0.2,	0.1,	0.2,	0,	0.2,	0.19, -0.1,	0.68,	0.58,	-0.19,	0,	-0.19,	0.39,	0.38,	0.1,	0,	0.1,	0.29, -0.48,	0.57,	0.48,	-0.47,	0.38,	0.09,	0.47,	0,	0,	-0.19,	0.19,	0.38, -0.56,	0.47,	0.28,	-0.19,	-0.09,	0.28,	0.28,	0,	0,	-0.28,	0,	0, -1.03,	0.85,	0.47,	0,	0.09,	-0.09,	0.19,	0,	-0.19,	0,	0.09,	-0.09, -0.84,	0.38,	0.75,	-0.37,	0.28,	0.09,	0.28,	0,	0.09,	0.19,	0.09,	0.74, -0.64,	0.65,	0.18,	0,	-0.18,	0.18,	0.37,	0.09,	0.09,	0,	0.27,	0.64)
mydata
# raw data
# -0.31	0.41	0.51	-0.2	0.61	0.2	0.61	-0.3	-0.1	-0.2	-0.51	0.41 -0.51	0.61	-0.2	0.1	-0.1	0.3	0	0.2	-0.3	0	-0.1	0.81 -0.6	0.4	0.5	0.1	-0.1	0	0.2	0.1	-0.1	0.1	0.1	0.6 -0.2	0.6	0.59	0	0	0.1	0.2	0.1	0.2	0	0.2	0.19 -0.1	0.68	0.58	-0.19	0	-0.19	0.39	0.38	0.1	0	0.1	0.29 -0.48	0.57	0.48	-0.47	0.38	0.09	0.47	0	0	-0.19	0.19	0.38 -0.56	0.47	0.28	-0.19	-0.09	0.28	0.28	0	0	-0.28	0	0 -1.03	0.85	0.47	0	0.09	-0.09	0.19	0	-0.19	0	0.09	-0.09 -0.84	0.38	0.75	-0.37	0.28	0.09	0.28	0	0.09	0.19	0.09	0.74 -0.64	0.65	0.18	0	-0.18	0.18	0.37	0.09	0.09	0	0.27	0.64

# timestamp
# mydata
plot.ts(mydata)

# ts() function - ts data load
germaninfl=ts(mydata,start=2008,frequency = 12)
plot(germaninfl)

# basic structure
ts(mydata, start=c(2008,3),frequency=12)
```

## Lecture 16: Mission statement
Seaonal time series data, we are focusing on (1) seasonality, and (2) cross-validation.Primarily, we can inport data by:ts(data, start=, frequency=)

This dataset is a monthly inflation rate of Germany, class=ts, frequency=!2, no trend with seasonal pattern. For dealing with seasonality, we should consider (1) seasonal decomposition, (2) exponential smooothing and (3) seasonal ARIMA. 

1. **Seasonal decomposition**
`decompose()` function is not allowed for the forecast. STL + ETL enables us to decompose for the forecast, which can be resembled by `stlf()`. 

2. **Sesonal ARIMA**
ARIMA(1,0,2)(0,1,1) or `auto.arima()` function

3. **Exponential smoothing**
get an automated exponential smoothing model by `est()` - automated or `hw()`.

Do these models have high accuracy? 
To answer this question, we should splot test/training set, and corss validate by `tsCV()`.

## Lecture 17: Project II Script
```{r}

# Data load
# mydata=scan()
# plot.ts(mydata)

# germaninfl=ts(mydata,start=2008,frequency = 12)
# plot(germaninfl)

# Seasonal decomposition
plot(germaninfl)
decompose(germaninfl)
plot(decompose(germaninfl))

# using the stl method
plot(stl(germaninfl,s.window = 7))

# stl forecasting
library(forecast)
plot(stlf(germaninfl,method="ets"))

# comparison with a standard est forecast
plot(forecast(ets(germaninfl),h=24))

# using autoplot
library(ggplot2)
autoplot(stlf(germaninfl,method="ets"))

# Seasonal Arima(package forecast)
auto.arima(germaninfl,stepwise=T,
           approximation=F,trace=T)

# Getting an object
germaninflarima = auto.arima(germaninfl,stepwise=T,
                             approximation = F,trace=T)

# forecast
forec=forecast(germaninflarima)
plot(forec)

# exponential smoothing with ets
# auto generated
ets(germaninfl)
# forecast plot
germaninflets=ets(germaninfl)
plot(forecast(germaninflets,h=60))

# comparison with seasonal Holt Winters model
plot(hw(germaninfl,h=60))

# cross validation of 2 models
germnainflets=ets(germaninfl)
germaninflarima=auto.arima(germaninfl,
                           stepwise=T,
                           approximation=F,
                           trace=T)

## Function definition
forecastets = function(x,h){
  forecast(ets(x),h=h)
}

forecastarima = function(x,h){
  forecast(auto.arima(x),stepwise=T,approximation=F,h=h)
}

esterror=tsCV(germaninfl,forecastets,h=1)
arimaerror=tsCV(germaninfl,forecastarima,h=1)

mean(esterror^2,na.rm=T)
mean(arimaerror^2,na.rm=T)
```

## Lesson 18: Seasonal Decomposition
Date:7/17
Seasonal decomposition is an old analytical method, but useful for dividing the data into its components:
- trend
- seasonality
- remainder/white noise

The seasonal decomposition is an additive/multiplicative model, and we can get q quick idea about the data. 

Note that there are several drawbacks:
(1) First few observations are NA
(2) Slow to catch fast rises
(3) Adopts a constant seasonal component

The basic functions are: `decompose()` and `stl()`.
```{r}
decompose(germaninfl)
plot(decompose(germaninfl))
```

Overall there is no clear trend of German inflation rate, however, from the seasonality (second from the below), we can see some pattern (Christmas period - high inflation due to mass parchasing behavior). 

The simple decomposition method is helpful, but the new methods for seasonal decomposition are:
- `x11()`
- `Seats()`
- `STL()`

As the benefits of the above new methods, they don't require omitting values (i.e., No NA values), and the seasonal part can be adjusted over time. 

(1) STL function
- Seaonsla and trend decomposition with LOESS
- Function of R base/data=time series
- Robust towards outlier
- Suitable toward an additive model (For multiplicative model, data transaformation is needed)
- Seaonanl and trend cycle may change overtime

Example:
stl(data,s.window=7)
- `s.window=`: number of required seasonal cycles to calculate changes for the seasonality (e.g., x>=7)
- `t.window=`: number of required seasonal cycles to calculate changes for the trend

```{r}
# plot visualization
plot(stl(germaninfl,s.window = 7))
# second to top: seasonality
```


### Create a forecast with STL decomposition
There are two approaches 
<Approach 1>
1. Use `stl()` on the data
2. Put the results into an object
3. Feed the objection into the `forecast.stl()` function

<Alternative approach 2>
1. Use the `stlf()` function on the data
2. USe the `method` argument

```{r}
library(forecast)

# stl forecasting + ETS
plot(stlf(germaninfl,method="ets"))

# comparison with a standard ets forecast
# peaks and slppes are different
plot(forecast(ets(germaninfl),h=24))

# using autoplot
library(ggplot2)
autoplot(stlf(germaninfl,method="ets"))
```

## Lecture 19: Seasonal ARIMA

ARIMA model can be applied to seasonal data. 
For setting, there are two methods.
<Manual method>
1. Manual parameter selection with `acf()` and `pacf()`
2. Differencing the dataset
3. Feed the data and the parameters into an ARIMA function

<Auto method>
1. Feed the time series into the `auto.arima()` function ("forecast" package)
2. Fine tune the model for best results.

Seasonal ARIMA models have two sets of parameters: 
A regular set and a second set for the seasonal part

ARIMA (p,d,q) (P,D,Q) [m]
- first(p,d,q):non-seasonal part
- second(P,D,Q):seasonal part
- third(m): frequency

```{r}
# Seasonal ARIMA (package forecast)
library(forecast)
auto.arima(germaninfl,stepwise=T,
           approximation=F,trace=T)
```

ARIMA(*1*,0,*2*) (0,1,1)[12] is selected as the best model. 
1. *auto correlation 1* coefficient -0.7947
2. Moving average *1* coefficient 0.7786
3. Moving average *2* coefficient 0.215

```{r}
# Getting an object <forecast>
germaninflarima <- auto.arima(germaninfl,
                              stepwise=T,
                              approximation=F,
                              trace=T)
forec <- forecast(germaninflarima)
plot(forec)
```

ARIMA is one option for seasonal data set. 
When we have such a data, ETS model, and Holt-Winters model should be considered together. 

## **Lecture 20: Exponential Smooothing with `ETS()`**
Project II

Modeling with exponential smooothing is:
- Holt-winters seasonal method `hw()`
- ETS method `ets()`

Parameters are $\alpha$(error),$\beta$(trend),$\gamma$(seasonality).

Depending on the dataset, we might not need to consider seasonality. Generally, smoothing condition is categorized based on the scale of x:
- Smooth model (older data is considered, x close to 0)
- Reactive model (Heavily relies on recent data, x close to 1)


Activate the `forecast` package
Exponential smooothing function iwth automated parameter seection, data= time series, model="ZZZ" automated parameter selection. 
```{r}
# library(forecast)
# est(data)
# # Function arguments
?ets()
```

The value of the model arguments are:
- Z: Auto generated
- A: Additive
- M: Multiplicative
- N: Non-existent

Each parameter of the model can be set to one of above values. The error must be existent. 

For example, model=="MZM" is going to be:
multiplicative error, auto selected trend, multiplicative seasonality. 


```{r}
# Exponentila smoothing with ets
# Auto generated
library(forecast)
ets(germaninfl)

# forecast plot
germaninflets=ets(germaninfl)
plot(germaninflets)     
plot(forecast(germnainflets,h=60))

# comparison with seasonal Holt Winter model
plot(hw(germaninfl,h=60))
```

## **Lecture 21: Time series cross validation**

### **How to compare time series model**
Infomration criteria is not a sufficinet base to compare different systems of models. Forecast accuracy is measured by mean square error (MSE). We use function `tsCV()` from the `forecast`package. 

However, the computation of errors might take long. Alternative function could be `cvar()` for autoregressive neural nets.

In calculating the error rate, producing an error rate for each time point is the series
- actual value vs calculated value

Training and test sets
- Test = 1 observation
- Training = all observations before the test obs

The error rates are comuputed along timeline (rolling forecast origin). The number of step-ahead forecasts can be specified (`window` argument)

The forecast accuracy is average value of the all the errors of the whole time series. 

Mdels and forecasts we prodocued are: 
1. Seasonal Decomposition `stlf()`
2. Seasonal ARIMA `auto.arima()`
3. Exponential Smoothing `ets()`

Steps to proceed
1. Create the function for the model
2. Feeding the time series and the model function into the `tsCV()` function
3. Generate the mean of the error rates

**Function for the ETS model**
- x=dataset
- h=forecast length
- `est()` function from the forecast package
```{r}
forecastes=
  function(x,h)
  {forecast(est(x),h=h)}
```

**Function for the ARIMA model**
- x=dataset
- h=forecast length
- `auto.arima()` function from the `forecast` package
- Getting the most accurate model possible with `stepwise=T` and `approximation=F`

```{r}
library(forecast)
forecastarima=
  function(x,h)
  {forecast(auto.arima(x),
            stepwise=T,
            approximation=F,
            h=h)
  }
```


We also need to defice cross validation. 

```{r}
etserror=tsCV(germaninfl,forecastets,h=1)
arimaerror=tsCV(germaninfl,forecastarima,h=1)
```

The `tsCV()` results in a vector containing errors between forecasts and actual values. 

We choose a metric of forecast accuracy
- RMSE: Root mean squared error
- MSE: Mean squared error

The difference between two errors, root of the results or not. The better model with lower error is the ARIMA model.
```{r}
# error calculation
mean(etserror^2,na.rm=T)
mean(arimaerror^2,na.rm = T)
```

Alternatively, we can go with the following options. 
1) Training (80) + Test Set (20) + `accuracy()`
2) Time series cross validation with `tsCV()`
 
# Section 4: Project III Irregularly Spaced Data: Analyze A Biotech Stock

## Lecture 22: Mission Statement
In the project III, we focus on 1) Scraping data from Yahoo Finance, and 2) Processing of real world data.

Step 1) Getting the dataset
By `quantmod` lirary, we use the function `getSymbols()` and its variations for scraiping data from various sources for the Novartis stock (NVS)

Novartis Stock (NVS)
01.Jan 2015 - 01.Jan 2016

### Getting familar with the dataset
What is the general dataset structure?
Which class is the dataset?
What are the potential problems?
Get some initial models - e.g., ARIMA, ETS?
Do forecastng on the original dataset
Compare the models - How do they perform against each other? 

__Key Question__
Are there any patterns between the traiding theys which could be exploited? 

Workplan 
- `data.frame` format
- data is recognized as date value
- Identifying missing days
- Delete the weekends: frequency=5
- Impute the NAs - `na.x()` family of functions (e.g., `na.locf()`)
- Answering the main analytical question with plots (`monthplot()` and `seasonplot()` with the highest and lowest prices).

## LEcture 23：scraping the Data from Yahoo Finance

1) Scraping data through API
 - Bloomberg
 - Google finance
 - Yahoo Finance
 - Oanda
 - Federal Reserve

2) Outsourcing Tasks through API
 - Tableau

Quantiative Financial Modeling Framework
- install and activate the package `quantmod`
```{r}
# install.packages("quantmod")
library(quantmod)
```

Function family `getSymbols()` 
no API connection is required, in the getSymbols()
- argument `src =""`: "google","yahoo","oanda","MySQL"
- variation of `getSymbols()`: getSymbols.google(), getSymbols.yahoo(), getSymbols.MySQL()

```{r eval=FALSE}
# activate the package "quantmod"
library(quantmod)

# name of the new object
# novartis = getSymbols(
#     # function for stock data scraping
#     symbol="NVS",
#     #stock symbol - ticker for Novartis stock
#     # scraping more than one stock, symbol=c("","")
#     auto.assign = F,
#     # assign an object name automatically - FALSE for manual assignemnt of single stack, while scraping multiple stocks this must be TRUE
#     from="2015-01-01",
#     to="2016-01-01")

novartis = getSymbols("NVS", auto.assign=F, 
                   from = "2015-01-01", to = "2016-01-01")

sessionInfo()
getSymbols("AAPL")
getSymbols("OIH", src = "yahoo", auto.assign=FALSE)[,1:5]
w <- curl::curl_fetch_memory("https://finance.yahoo.com",curl::new_handle(verbose=TRUE))

# View(novartis)
```

`xts` refers to extensible time series data. The feature of this data type is:
- multivariate format
- rowID = Date
- Data is available only for tranding days
- Weekend and Holidays are not present
- Irregular spaced dataset
 - Frequency cannot be asigned
 - Seasonal patterns cannot be identified 

We plot the data
```{r eval=FALSE}
# data visualization
plot(as.ts(novartis$NVS.Open))
```


## Lecture 24: Exploring the data

Getting started with irregular spaced time series data, most time series functions and models don't work effectively on irregularly spaed data. Some teqniques help to get a basic ide about the data even if irregulaly spaced.

The first option is plot the data
```{r eval=FALSE}
# activate quantmod library
library(quantmod)

# Function chartSeries() for plotting quantmod derived financial time series data 
 # data = time series data
 # type = chart type ("auto","line","bars","candlesticks","matchsticks")
chartSeries(data=novartis,type="line")

# ac and pacf to get an idea about auto-
library(forecast)
ggtsdisplay(novartis$NVS.Open)
```

```{r eval=FALSE}  
# ARIMA model
novartisarima = auto.arima(novartis$NVS.Open,
                           stepwise=T,
                           approximation=F,
                           trace=T);novartisarima
# ARIMA (0,1,1) is obtained - data seems randomly distributed
# AIC=895, several models were tested and AIC of the optimal model is minimal

# Alternative ARIMA with autogregressive part
novartisarima2 = Arima(novartis$NVS.Open,order=c(1,1,1))
novartisarima2
```


```{r eval=FALSE}
# Forecast arima
plot(forecast(novartisarima,h=20))
plot(forecast(novartisarima2,h=20))
```

The feature of the above optimal model's plot is;
- MA (moving average) with short window
- Almost like an LOCF (last observation carried foward) model
- No clear trend
- No seasonality

When we see the alternative **ARIMA** model, it is not that different. The ARIMA models assume that the last observation is indicative of the next one. Prior obsevations don't matter much.

Then, we construct a **ETS(exponential smooothing)** model for Novatis stock data. It does not look different, the model is based on M (multipled initial level), no trend nor seasonality, several of the last observations are considered. 
```{r eval=FALSE}
# ETS model
novartisets = ets(novartis$NVS.Open)
# Forecast model
plot(novartisets, h=20)
```

As a conclusion, clear and actionable patterns are hard to extract.Tiny advantages=tiny gains, going deeper with the analysis
1) Is there a difference in the day of the week?
2) Are rises expected on a specific week day?
3) Which months are the most promising ones for certain strategies?

For the above things, pre-processing is required to answer those type of question, and converted to regular time series data.

## Lecture 25: Project III Script
```{r eval=FALSE}
# Fetching data from yahoo with quantmod
library(quantmod)

novartis = getSymbols("NVS", auto.assign=F, 
                  from = "2015-01-01", to = "2016-01-01")
# use argument return.class to modify output class to ts or mts

## using a column like a standard ts
plot(as.ts(novartis$NVS.Open))

# functions to explore unprocessed xts from quantmod
chartSeries(novartis, type = "line")

# acf and pacf to get an idea about autocorrelation
library(forecast)
ggtsdisplay(novartis$NVS.Open)

# Arima model
novartisarima = auto.arima(novartis$NVS.Open, 
                           stepwise = T, 
                           approximation = F, 
                           trace = T); novartisarima

# Alternative arima with autoregressive part 
novartisarima2 = Arima(novartis$NVS.Open, order = c(1,1,1))
novartisarima2

# Forecast arima
plot(forecast(novartisarima, h = 20))
plot(forecast(novartisarima2, h = 20))

# Ets model
novartisets = ets(novartis$NVS.Open)

# Forecast ets
plot(forecast(novartisets, h = 20))

## Getting a regular time series

# Conversion to dataframe
novartis = as.data.frame(novartis)

### Adding the rownames as date
novartis$Date = rownames(novartis)
novartis$Date = as.Date(novartis$Date)
head(novartis)

# Creating the date column, 'by' can be either nr days or integer
# 'from' and 'to' with as.Date to make sure this is a date format
mydates = seq.Date(from = as.Date("2015-01-01"), 
                to = as.Date("2016-01-01"), 
                by = 1)

# Converting to a df (required for the merge)
mydates = data.frame(Date = mydates)

# Padding with 'mydates'
mydata = merge(novartis, mydates, by = "Date", all.y = T)

# Removing initial days to start on monday
mydata = mydata[5:366,]

# Removing sundays, watch the from as the first one to remove
mydata = mydata[-(seq(from = 7, to = nrow(mydata), by = 7)),]
# Removing saturdays
mydata = mydata[-(seq(from = 6, to = nrow(mydata), by = 6)),]

# Using last observatoin carried forward imputation
mydata = na.locf(mydata)

## Which days are the ones best to buy or sell?

# Putting the closeprice into a weekly time series
highestprice = ts(as.numeric(mydata$NVS.High), 
                  frequency = 5)

# Various plots
seasonplot(highestprice, season.labels = c("Mon", "Tue", "Wed", "Thu", "Fri"))
monthplot(highestprice)
monthplot(highestprice, base = median, col.base = "red")
plot(stl(highestprice, s.window = "periodic"))

# Comparison with the low prices
par(mfrow = c(1,2))
lowestprice = ts(as.numeric(mydata$NVS.Low), 
                 frequency = 5)
monthplot(lowestprice, base = median, col.base = "red")
monthplot(highestprice, base = median, col.base = "red")
par(mfrow = c(1,1))
```

## Lecture 26: Getting a Regular Time Series

1) Data vs Analytical Goals
- Irregularly spaced data 
e.g., Holidays and weekends are missing, US traiding days only
- Analytical questions
e.g., Is there any pattern in the trading days?

2) Conveting the data into the Regularly spaced time series 
(A) Convert the original data into a data frame, and (B) Create a second data.frame with a calender days result in merging both data frames to the *regular spaced time series with NAs*


### Conversion to a data frame
```{r eval=FALSE}
novartis=as.data.frame(novartis)
```


### Add the row names as date
```{r eval=FALSE}
novartis %>% 
  mutate(Date=rownames(novartis)) %>% 
  as.Date(Date)
head(novartis)
```


- Name of the object to be created
- Sequence of data values
- Parameters "from" and "to" to specify the time window
 - data class=as.Date("")
- Parameter "by" to specify the step size

```{r eval=FALSE}
# creating the date column, "by" can be either nr days or 
# "from" and "to" with as.Date to make sure this is a date 

mydates=
seq.Date(
  from=as.Date("2015-01-01"),
  to=as.Date("2016-01-01"),
  by=1)

# converting to a df(tibble)
library(tidyverse)
mydates=tibble(Date=mydates)
```

### Padding with "mydates" by `merge()`
- merting the two data frames
- identifying missing dates and marking them as NA
- Name of the new data frame
- Merging function of R Base (alternatively, join functions from `dplyr` package)
 - First object(X)
 - Second object(Y)
- Name of the column to merge by
- All rows of object Y get included
 - Results in a full join of the two data frames
 
```{r eval=FALSE}
mydata = merge(novartis, mydates,by="Date",all.y=T)
```

### Removing initial days to start on Monday
 - removing Saturdays and Sundays
 - Impute holiday prices with `na.locf()`
 
 First of all subset the data
```{r eval=FALSE}
mydata=mydata[5:366,]

# Removing Sundays, watch the from 
mydata <- 
# Removing Saturday
mydata <- 
  
```

For the above Holiday extraction, we use seq()
- substracte the sequence from mydata
- sequence generation function of R base: `seq()`
- `Form` = the first observation to be removed
- `To`= the last observation of the dataset
```{r eval=FALSE}
mydata =
  mydata[-(
    seq(
      from=7,
      to=nrow(mydata),
      by=7)),]
```
  
To confirm the process:
1) Converting the original dataset into a data frame
2) Create a second data frame with all the calender days
3) Merging the two data frames
4) Remove all the Saturdays and Sundays
5) Impute Holiday prices with a last observation carried foward method


## Lecture 27: Visually analyzing the data

We convert the time-series data with frequency five.

- Extracting the highest prices
- Name of the `ts` object to be created
- Time series, convert the data to numeric
- frequency to five

```{r}
# Putting the close price into a weekly time series
library(tidyverse)

# supplement sample data
mydata <- tibble(
  High=c(2.5,3,3.5,4,4,3,5,5.5,
  6,7,13,25,26,26.5,25,25.5,27,29,30)
)
mydata


highestprice =
  ts(
    as.numeric(mydata$High),
    frequency=5
  )
plot(highestprice)
```

Plotting the weeks against each other, two functions are useful: `seasonaplot()` and `ggseasonaplot()`.
```{r}
library(forecast)

seasonplot(highestprice,season.label=c("Mon","Tue","Wed","Thu","Fri"))
monthplot(highestprice)
monthplot(highestprice,base=median,col.base="red")
plot(stl(highestprice,s.window="periodic"))


# comparison with the low prices
mydata1 <- tibble(
  Low=c(2,2.3,2.5,4.5,3,3.5,3,4.5,
  5,6,12.5,23,25,25.5,24,25,25,27,28)
)
mydata2 <- bind_cols(mydata1,mydata)
mydata2
lowestprice =ts(as.numeric(mydata2$Low),
                frequency=5)

# data plot check
par(mfrow=c(1,2))
monthplot(lowestprice,base=median,col.base="red")
monthplot(highestprice,base=median,col.base="red")
par(mfrow=c(1,1))
```

# Section 5: Project IV Neural Networks: Neural Nets and Interactive Graphs
## Lecture 39: Mission Statement

**Step 1: Data pre-processing**
1) Clean the data, as real world date often needs to be cleaned prior to analysis. People collecting data are not always trained in data science. 
2) A time consuming process
The goal is to prepare a numeric time series format, missing data imputed, and outlier detection and replacement.
3) Tools
`tidyr` package - `separate()`
`forecast` package - `tsclean()`

 
**Step2: Modeling the data**
1) Neural network model
forecast 36 months using `forecast` package - `nnetar()`, Seasonal changes 

**Step3: Data visualization using the Interactive chart**
1) Interative data visualization
Plot the model and the forecast, using interactive chart (zoom-in tool, reveal more detail upon hovering) - `dygraphs` package
2) Further option R Shiny

## Lecture 29: Getting familar with ras dataset


